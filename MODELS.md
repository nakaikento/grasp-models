# MODELS.md - Grasp ASR/翻訳モデル調査

最終更新: 2026-02-10

## ASRモデル比較

### 評価データ
- **ソース**: TED Talk 日本語 (高橋晋平「しりとり発想法」)
- **長さ**: 約5.5分 (331秒)
- **参照**: 日本語手動字幕 (1,986文字)

### 結果

| モデル | サイズ | 処理時間 | CER | 評価 |
|--------|--------|----------|-----|------|
| sherpa-onnx Korean (int8) | ~70MB | 高速 | 62.3% | ❌ 実用困難 |
| sherpa-onnx ReazonSpeech (FP32) | ~200MB | 高速 | 53.6% | ❌ 実用困難 |
| Whisper base | 74MB | ~20s (CPU) | 20.8% | 🟠 許容範囲 |
| Whisper small | 461MB | ~52s (CPU) | 17.8% | 🟡 良好 |

### CER (Character Error Rate) 目安
- < 10%: 優秀（ほぼ完璧）
- < 20%: 良好
- < 30%: 許容範囲
- > 30%: 要改善

### 所見

#### sherpa-onnx系 (Zipformer Transducer)
- **利点**: 軽量、高速、ストリーミング対応
- **欠点**: CER 50-60%台で精度が低い
- **備考**: リアルタイム処理向けだが、現状の精度では翻訳品質に影響大

#### Whisper系
- **利点**: 高精度（CER 17-21%）
- **欠点**: モデルサイズ大、処理時間長い
- **備考**: 
  - baseでも「こんにちは」→「この日は」等の誤認識あり
  - smallで改善されるが、モバイルリアルタイムには重い

### 誤認識例

**Whisper base:**
- 「こんにちは」→「この日は」
- 「アイデアを」→「間を」
- 「こう言われました」→「こういうわれました」

**Whisper small:**
- 上記の誤りが修正された

---

## 翻訳モデル比較

### 評価データ
- **ソース**: OpenSubtitles KO→JA (1,000サンプル)
- **指標**: chrF++ (主), BLEU (参考)

### 結果

| モデル | chrF++ | BLEU | 速度 | 備考 |
|--------|--------|------|------|------|
| M2M100-418M | 14.9 | 6.6 | 遅い | 品質不足 |
| **Qwen2.5-7B** | **49.3** | 26.8 | 37行/s (RTX4090) | 採用 |

### 所見
- Qwen2.5-7Bが圧倒的に高品質（chrF++ 49 vs 15）
- 現在1.19M行の教師データ生成中（RunPod）

---

## 現行モデル構成 (grasp-ko-ja)

### 日本語ASR
- **モデル**: sherpa-onnx-zipformer-ja-reazonspeech-2024-08-01
- **量子化**: int8
- **CER**: 約54% (要改善)

### 韓国語ASR
- **モデル**: sherpa-onnx-streaming-zipformer-korean-2024-06-16
- **量子化**: int8
- **CER**: 約62% (要改善)

### 翻訳 (JA→KO, KO→JA)
- **モデル**: MarianMT (量子化ONNX)
- **品質**: 要評価

---

## 改善案

### 短期
1. Whisper tiny/baseのモバイル最適化を検討
2. sherpa-onnxモデルのファインチューニング

### 中期
1. Whisper.cpp / whisper-onnx でモバイル対応
2. 教師データでMarianMTファインチューニング

### 長期
1. 独自ASRモデルのトレーニング（ドメイン特化）
2. End-to-End Speech Translation
