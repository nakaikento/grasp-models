# mt-ja-ko

æ—¥æœ¬èªâ†’éŸ“å›½èªã®ã‚ªãƒ³ãƒ‡ãƒã‚¤ã‚¹ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã€‚å­—å¹•ãƒ»ãƒ‰ãƒ©ãƒãƒ»ã‚¢ãƒ‹ãƒ¡ã«ç‰¹åŒ–ã—ãŸè»½é‡é«˜é€Ÿã‚¨ãƒ³ã‚¸ãƒ³ã€‚

## èƒŒæ™¯

Helsinki-NLP (Hugging Face) ã«æ—¥éŸ“ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ãªã‹ã£ãŸãŸã‚ã€è‡ªä½œã—ã¾ã—ãŸã€‚

## ãƒ¢ãƒ‡ãƒ«æ€§èƒ½

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| **Test BLEU** | 33.03 |
| Final Loss | 0.97 |
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | 61M |

â€» BLEU 30-40 ã¯å•†ç”¨ç¿»è¨³ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆGoogleç¿»è¨³ç­‰ï¼‰ã«åŒ¹æ•µã™ã‚‹ãƒ¬ãƒ™ãƒ«

## ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º

| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | ã‚µã‚¤ã‚º | ç”¨é€” |
|------------|--------|------|
| PyTorch (fp32) | ~245MB | å­¦ç¿’ãƒ»è©•ä¾¡ |
| ONNX (fp32) | 572MB | æ¨è«– |
| **ONNX (INT8é‡å­åŒ–)** | **148MB** | **Androidæ¨å¥¨** |

### ONNX INT8 å†…è¨³

| ãƒ•ã‚¡ã‚¤ãƒ« | ã‚µã‚¤ã‚º |
|----------|--------|
| encoder_model_quantized.onnx | 35MB |
| decoder_model_quantized.onnx | 57MB |
| decoder_with_past_model_quantized.onnx | 54MB |
| spm.model | 807KB |

## å­¦ç¿’è¨­å®š

| é …ç›® | å€¤ |
|------|-----|
| ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | MarianMT (Transformer) |
| Encoder/Decoder Layers | 6 / 6 |
| d_model | 512 |
| Attention Heads | 8 |
| Vocab Size | 32,000 (SentencePiece) |
| å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ | OPUS OpenSubtitles (~55ä¸‡ãƒšã‚¢) |
| GPU | NVIDIA RTX 4090 |
| ãƒãƒƒãƒã‚µã‚¤ã‚º | 128 |
| ã‚¨ãƒãƒƒã‚¯ | 10 |
| å­¦ç¿’æ™‚é–“ | ç´„50åˆ† |

## ç¿»è¨³ä¾‹

```
ğŸ‡¯ğŸ‡µ ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ
ğŸ‡°ğŸ‡· ì•ˆë…•í•˜ì„¸ìš”?

ğŸ‡¯ğŸ‡µ é€ƒã’ã‚ï¼
ğŸ‡°ğŸ‡· ë„ë§ì³!

ğŸ‡¯ğŸ‡µ å›ã®ã“ã¨ãŒå¥½ãã ã€‚
ğŸ‡°ğŸ‡· ë„ˆì— ëŒ€í•´ ì¢‹ì•„í•´

ğŸ‡¯ğŸ‡µ çµ¶å¯¾ã«è«¦ã‚ãªã„ã€‚
ğŸ‡°ğŸ‡· ì ˆëŒ€ í¬ê¸°í•˜ì§€ ì•Šì„ê±°ì•¼.

ğŸ‡¯ğŸ‡µ å›ãŒã„ãªã„ã¨ç”Ÿãã¦ã„ã‘ãªã„ã€‚
ğŸ‡°ğŸ‡· ë„¤ê°€ ì—†ìœ¼ë©´ ì‚´ì•„ê°ˆ ìˆ˜ ì—†ì–´.
```

## ä½¿ç”¨æ–¹æ³•

### Python (ONNX Runtime)

```python
from optimum.onnxruntime import ORTModelForSeq2SeqLM
import sentencepiece as spm

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
model = ORTModelForSeq2SeqLM.from_pretrained("models/ja-ko-onnx-int8")

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
sp = spm.SentencePieceProcessor()
sp.load("models/ja-ko-onnx-int8/spm.model")

# ç¿»è¨³
text = "ã“ã‚“ã«ã¡ã¯"
inputs = sp.encode(text, out_type=int)
# ... generate ...
```

### Android (sherpa-onnx)

ONNX Runtime + NNAPI ã§ Tensor G2/G3 ã®NPUã‚’æ´»ç”¨å¯èƒ½ã€‚

---

## ğŸ“˜ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ±ç”¨ç‰ˆï¼‰

`generate_teacher_data.py` ã‚’ä½¿ç”¨ã—ã¦ã€NLLB-200ãƒ¢ãƒ‡ãƒ«ã§é«˜å“è³ªãªæ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚

### æ—¥æœ¬èª â†’ éŸ“å›½èª

```bash
python3 training/generate_teacher_data.py \
  --src_lang ja \
  --tgt_lang ko \
  --src_file data/splits/train.ja \
  --output_file data/teacher/train_ja_ko.ko \
  --batch_size 40 \
  --num_beams 3
```

### éŸ“å›½èª â†’ æ—¥æœ¬èª

```bash
python3 training/generate_teacher_data.py \
  --src_lang ko \
  --tgt_lang ja \
  --src_file data/splits/train.ko \
  --output_file data/teacher/train_ko_ja.ja \
  --batch_size 40 \
  --num_beams 3
```

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--src_lang`: ã‚½ãƒ¼ã‚¹è¨€èª (`ja` | `ko`)
- `--tgt_lang`: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª (`ja` | `ko`)
- `--src_file`: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
- `--output_file`: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
- `--model_name`: NLLBãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `facebook/nllb-200-3.3b`ï¼‰
- `--batch_size`: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 40ï¼‰
- `--num_beams`: ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®å¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
- `--max_length`: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰

---

## ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰æ‰‹é †

ã‚¼ãƒ­ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹å®Œå…¨ãªæ‰‹é †ã§ã™ã€‚

### ç’°å¢ƒæ§‹ç¯‰

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/nakaikento/mt-ja-ko.git
cd mt-ja-ko

# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv venv
source venv/bin/activate

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

### Step 1: ãƒ‡ãƒ¼ã‚¿å–å¾—

[OPUS OpenSubtitles](https://opus.nlpl.eu/OpenSubtitles-v2018.php) ã‹ã‚‰æ—¥éŸ“å¯¾è¨³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€`data/raw/` ã«é…ç½®ã—ã¾ã™ã€‚
```bash
ls data/raw/
# OpenSubtitles.ja-ko.ja  (~1.18Mè¡Œ)
# OpenSubtitles.ja-ko.ko  (~1.18Mè¡Œ)
```

â€» æ—¢ã«ãƒªãƒã‚¸ãƒˆãƒªå†…ã® `data/raw/` ã«ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

### Step 2: ãƒ‡ãƒ¼ã‚¿èª¿æŸ»ãƒ»åˆ†æ

ç”Ÿãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
# å†’é ­ç¢ºèª
python scripts/inspect_data.py

# è©³ç´°åˆ†æï¼ˆãƒã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã€é‡è¤‡ã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå“è³ªï¼‰
python scripts/analyze_data.py
```

**analyze_data.py ã®å‡ºåŠ›ä¾‹ï¼š**
- åŸºæœ¬çµ±è¨ˆï¼ˆè¡Œæ•°ã€æ–‡å­—æ•°åˆ†å¸ƒï¼‰
- ãƒã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆHTMLã‚¿ã‚°ã€éŸ³æ¥½è¨˜å·ã€ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆç­‰ï¼‰
- ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå“è³ªï¼ˆé•·ã•æ¯”ç‡ãŒæ¥µç«¯ãªãƒšã‚¢ï¼‰
- é‡è¤‡ãƒšã‚¢ã®æ¤œå‡º

### Step 3: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°

ãƒã‚¤ã‚ºé™¤å»ã€é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ã€é‡è¤‡é™¤å»ã‚’è¡Œã„ã¾ã™ã€‚

```bash
python scripts/clean.py
```

**å‡¦ç†å†…å®¹ï¼š**
| å‡¦ç† | èª¬æ˜ |
|------|------|
| ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å» | HTMLã‚¿ã‚°ã€â™ªè¨˜å·ã€å­—å¹•ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã€URLç­‰ |
| ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ– | é€£ç¶šè¨˜å·ã®æ•´ç†ã€ç©ºç™½ã®æ­£è¦åŒ– |
| é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ | 4æ–‡å­—æœªæº€ã€100æ–‡å­—è¶…ã‚’é™¤å¤– |
| é•·ã•æ¯”ç‡ãƒ•ã‚£ãƒ«ã‚¿ | æ—¥éŸ“ã®æ–‡å­—æ•°æ¯”ç‡ãŒ0.25ã€œ4.0ã®ç¯„å›²å¤–ã‚’é™¤å¤– |
| é‡è¤‡é™¤å» | å®Œå…¨ä¸€è‡´ãƒšã‚¢ã®é‡è¤‡ã‚’å‰Šé™¤ |

**å‡ºåŠ›ï¼š**
```
data/cleaned/
â”œâ”€â”€ cleaned.ja    # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿æ—¥æœ¬èª
â”œâ”€â”€ cleaned.ko    # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿éŸ“å›½èª
â””â”€â”€ stats.txt     # çµ±è¨ˆæƒ…å ±
```

### Step 4: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å­¦ç¿’

æ—¥éŸ“ä¸¡è¨€èªã‚’å«ã‚€SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

```bash
python scripts/train_tokenizer.py
```

**è¨­å®šï¼š**
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ |
|-----------|-----|
| vocab_size | 32,000 |
| model_type | unigram |
| character_coverage | 0.9995 |

**å‡ºåŠ›ï¼š**
```
data/tokenized/
â”œâ”€â”€ spm.model    # SentencePieceãƒ¢ãƒ‡ãƒ«
â””â”€â”€ spm.vocab    # èªå½™ãƒ•ã‚¡ã‚¤ãƒ«
```

### Step 5: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²

train / val / test ã«åˆ†å‰²ã—ã¾ã™ã€‚

```bash
python scripts/split_data.py
```

**åˆ†å‰²æ¯”ç‡ï¼š**
| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ã‚µã‚¤ã‚º |
|-------------|--------|
| Train | ~99% |
| Val | 5,000 |
| Test | 5,000 |

**å‡ºåŠ›ï¼š**
```
data/splits/
â”œâ”€â”€ train.ja, train.ko
â”œâ”€â”€ val.ja, val.ko
â””â”€â”€ test.ja, test.ko
```

### Step 6: çŸ¥è­˜è’¸ç•™ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

å¤§è¦æ¨¡Teacherãƒ¢ãƒ‡ãƒ«ï¼ˆNLLB-200ç­‰ï¼‰ã‹ã‚‰soft labelsã‚’ç”Ÿæˆã—ã¾ã™ã€‚

```bash
python training/generate_teacher_data.py
```

### Step 7: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’

MarianMTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
# GPUæ¨å¥¨ï¼ˆRTX 4090ã§ç´„50åˆ†ï¼‰
python training/train.py
```

**å­¦ç¿’è¨­å®šï¼ˆconfig.pyï¼‰ï¼š**
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ |
|-----------|-----|
| batch_size | 128 |
| epochs | 10 |
| learning_rate | 5e-5 |
| warmup_steps | 1000 |

**å‡ºåŠ›ï¼š**
```
models/
â””â”€â”€ ja-ko-marianmt/
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ config.json
    â””â”€â”€ ...
```

### Step 8: ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXå½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚

```bash
python export/to_onnx.py
```

**å‡ºåŠ›ï¼š**
```
models/ja-ko-onnx/
â”œâ”€â”€ encoder_model.onnx
â”œâ”€â”€ decoder_model.onnx
â”œâ”€â”€ decoder_with_past_model.onnx
â””â”€â”€ spm.model
```

### Step 9: INT8é‡å­åŒ–

ãƒ¢ãƒã‚¤ãƒ«å‘ã‘ã«é‡å­åŒ–ã—ã¾ã™ã€‚

```bash
python export/quantize.py
```

**å‡ºåŠ›ï¼š**
```
models/ja-ko-onnx-int8/
â”œâ”€â”€ encoder_model_quantized.onnx      (35MB)
â”œâ”€â”€ decoder_model_quantized.onnx      (57MB)
â”œâ”€â”€ decoder_with_past_model_quantized.onnx (54MB)
â””â”€â”€ spm.model                         (807KB)
```

### Step 10: è©•ä¾¡

ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§è©•ä¾¡ã—ã¾ã™ã€‚

```bash
python training/evaluate.py
```

**è©•ä¾¡æŒ‡æ¨™ï¼š**
- BLEU
- chrF++
- æ¨è«–æ™‚é–“

---

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¦‚è¦å›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  OPUS OpenSubtitles (ja-ko)                                     â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ inspect_data â”‚ â† ç”Ÿãƒ‡ãƒ¼ã‚¿ç¢ºèª                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ analyze_data â”‚ â† å“è³ªèª¿æŸ»ãƒ»ãƒã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     1.18Mè¡Œ â†’ 1.03Mè¡Œ                              â”‚
â”‚  â”‚  clean  â”‚ â† ãƒã‚¤ã‚ºé™¤å»ãƒ»é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ãƒ»é‡è¤‡é™¤å»                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚         â”‚                                                       â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚         â–¼                  â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚ train_tokenizer â”‚  â”‚ split_data â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚         â”‚                  â”‚                                    â”‚
â”‚         â”‚    spm.model     â”‚   train/val/test                   â”‚
â”‚         â”‚                  â”‚                                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                  â–¼                                              â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚           â”‚   train   â”‚ â† MarianMTå­¦ç¿’                          â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                  â”‚                                              â”‚
â”‚                  â–¼                                              â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚           â”‚  to_onnx  â”‚ â† ONNXå¤‰æ›                              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                  â”‚                                              â”‚
â”‚                  â–¼                                              â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚           â”‚ quantize  â”‚ â† INT8é‡å­åŒ–                            â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                  â”‚                                              â”‚
â”‚                  â–¼                                              â”‚
â”‚           ONNX INT8 ãƒ¢ãƒ‡ãƒ« (148MB)                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
mt-ja-ko/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ inspect_data.py       # ç”Ÿãƒ‡ãƒ¼ã‚¿å†’é ­ç¢ºèª
â”‚   â”œâ”€â”€ analyze_data.py       # ãƒ‡ãƒ¼ã‚¿å“è³ªèª¿æŸ»
â”‚   â”œâ”€â”€ clean.py              # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
â”‚   â”œâ”€â”€ split_data.py         # train/val/teståˆ†å‰²
â”‚   â””â”€â”€ train_tokenizer.py    # SentencePieceå­¦ç¿’
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
â”‚   â”œâ”€â”€ config.py             # å­¦ç¿’è¨­å®š
â”‚   â”œâ”€â”€ evaluate.py           # è©•ä¾¡
â”‚   â””â”€â”€ generate_teacher_data.py  # çŸ¥è­˜è’¸ç•™ç”¨
â”œâ”€â”€ export/
â”‚   â”œâ”€â”€ to_onnx.py            # ONNXå¤‰æ›
â”‚   â””â”€â”€ quantize.py           # INT8é‡å­åŒ–
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆgitignoreï¼‰
â”‚   â”œâ”€â”€ cleaned/              # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿
â”‚   â”œâ”€â”€ tokenized/            # SentencePieceãƒ¢ãƒ‡ãƒ«
â”‚   â””â”€â”€ splits/               # train/val/test
â”œâ”€â”€ models/                   # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆgitignoreï¼‰
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.sh                  # ç’°å¢ƒæ§‹ç¯‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ run.sh                    # å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
â””â”€â”€ README.md
```

## ä»Šå¾Œã®æ”¹å–„ç‚¹

- [ ] å¦å®šè¡¨ç¾ï¼ˆã€œãªã„ã§ï¼‰ã®ç²¾åº¦å‘ä¸Š
- [ ] æ•¬èª/ã‚¿ãƒ¡å£ã®ä¸€è²«æ€§
- [ ] LoRAã«ã‚ˆã‚‹ã‚¸ãƒ£ãƒ³ãƒ«ç‰¹åŒ–ï¼ˆK-POPã€ã‚²ãƒ¼ãƒ ç­‰ï¼‰

## License

[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) - Non-commercial use only.

- âœ… å€‹äººåˆ©ç”¨ãƒ»å­¦ç¿’ç›®çš„ OK
- âœ… æ”¹å¤‰ãƒ»å†é…å¸ƒ OKï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆè¡¨è¨˜å¿…è¦ï¼‰
- âŒ å•†ç”¨åˆ©ç”¨ ä¸å¯

Note: å­¦ç¿’ã«NLLB-200ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€åŒãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«æº–æ‹ ã€‚
