# 📊 翻訳評価システム - 構造化フレームワーク

## 🎯 評価の目的

**ゴール:** 韓国語→日本語リアルタイム翻訳システムの品質向上

**解決すべき問題:**
1. どこにボトルネックがあるか？（ASR vs 翻訳）
2. 改善の優先順位は？
3. 改善効果を定量的に測定できるか？

---

## 📐 評価システムの構造

### レイヤー1: データ準備

```
┌─────────────────────────────────────────────────┐
│ データソース: TED Talks                          │
├─────────────────────────────────────────────────┤
│ 入力:                                           │
│  1. 音声ファイル (.wav)                         │
│  2. 韓国語字幕 (.vtt) ← ASR正解データ          │
│  3. 日本語字幕 (.vtt) ← 翻訳正解データ         │
│                                                 │
│ 取得方法:                                       │
│  download_ted.py <YouTube URL>                  │
│   ↓                                             │
│  data/audio/VIDEO_ID.wav                        │
│  data/subtitles/VIDEO_ID.ko.vtt                 │
│  data/subtitles/VIDEO_ID.ja.vtt                 │
└─────────────────────────────────────────────────┘
```

---

### レイヤー2: 3段階評価プロセス

```
┌──────────────────────────────────────────────────────────────┐
│ PHASE 1: ASR単独評価                                          │
├──────────────────────────────────────────────────────────────┤
│ 入力: 音声 (Korean)                                           │
│   ↓                                                          │
│ ASR Model (sherpa-onnx)                                      │
│   ↓                                                          │
│ 出力: 認識テキスト (Korean)                                   │
│   ↓                                                          │
│ 比較: 認識テキスト vs 韓国語字幕                              │
│   ↓                                                          │
│ 指標: CER (文字誤り率)                                        │
│       目標: < 10%                                            │
└──────────────────────────────────────────────────────────────┘
                          ↓
┌──────────────────────────────────────────────────────────────┐
│ PHASE 2: 翻訳単独評価（クリーンな入力）                       │
├──────────────────────────────────────────────────────────────┤
│ 入力: 韓国語字幕（正確なテキスト）                            │
│   ↓                                                          │
│ Translation Model (ko-ja-onnx)                               │
│   ↓                                                          │
│ 出力: 日本語翻訳                                              │
│   ↓                                                          │
│ 比較: 翻訳出力 vs 日本語字幕                                  │
│   ↓                                                          │
│ 指標: chrF++ (PRIMARY) - 文字+単語 n-gram                    │
│       目標: > 50                                             │
│       BLEU (BASELINE) - 業界標準                             │
│       目標: > 30                                             │
└──────────────────────────────────────────────────────────────┘
                          ↓
┌──────────────────────────────────────────────────────────────┐
│ PHASE 3: E2E評価（ASRエラー込み）                             │
├──────────────────────────────────────────────────────────────┤
│ 入力: 音声 (Korean)                                           │
│   ↓                                                          │
│ ASR Model                                                    │
│   ↓                                                          │
│ 認識テキスト（エラー含む）                                    │
│   ↓                                                          │
│ Translation Model                                            │
│   ↓                                                          │
│ 出力: 日本語翻訳（ASRエラーの影響あり）                       │
│   ↓                                                          │
│ 比較: E2E出力 vs 日本語字幕                                   │
│   ↓                                                          │
│ 指標: chrF++, BLEU                                           │
│       劣化度 = Phase2 - Phase3                               │
└──────────────────────────────────────────────────────────────┘
```

---

### レイヤー3: 評価指標の階層

```
┌─────────────────────────────────────────────────────────────┐
│ 評価指標ピラミッド                                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│                    🏆 GOLD STANDARD                         │
│                  ┌─────────────────┐                        │
│                  │   COMET-22      │                        │
│                  │ (人間評価 r=0.84)│                        │
│                  │  GPU必須・重い   │                        │
│                  └────────┬────────┘                        │
│                           │                                 │
│                  🎯 PRIMARY (実装済み)                       │
│              ┌───────────┴───────────┐                      │
│              │      chrF++          │                      │
│              │  文字+単語 n-gram     │                      │
│              │   目標: > 50         │                      │
│              │   軽量・高速         │                      │
│              └───────────┬───────────┘                      │
│                          │                                 │
│                📏 BASELINE (実装済み)                        │
│            ┌─────────────┴─────────────┐                   │
│            │         BLEU              │                   │
│            │      業界標準・比較用       │                   │
│            │       目標: > 30          │                   │
│            └───────────────────────────┘                   │
│                                                             │
│            🧠 SEMANTIC (オプション)                          │
│            ┌───────────────────────────┐                   │
│            │      BERTScore           │                   │
│            │     意味類似度評価        │                   │
│            │  環境依存・現在動作せず    │                   │
│            └───────────────────────────┘                   │
└─────────────────────────────────────────────────────────────┘
```

---

## 🔍 ボトルネック特定ロジック

### 意思決定フロー

```python
# 3段階評価の結果を取得
cer = Phase1_ASR_evaluation()
chrf_clean = Phase2_Translation_evaluation()
chrf_e2e = Phase3_E2E_evaluation()

# 劣化度を計算
degradation = chrf_clean - chrf_e2e

# ボトルネック特定
if cer > 15:
    ボトルネック = "ASR精度が低い"
    優先度 = "高"
    アクション = [
        "より良い音響モデルを探す",
        "韓国語特化のファインチューニング",
        "VAD閾値の調整"
    ]
    
elif chrf_clean < 45:
    ボトルネック = "翻訳品質が低い"
    優先度 = "高"
    アクション = [
        "より大きなモデル（MarianMT → mBART）",
        "追加の学習データ",
        "ビームサーチの調整"
    ]
    
elif degradation > 10:
    ボトルネック = "ASRエラーが翻訳に悪影響"
    優先度 = "中"
    アクション = [
        "翻訳モデルのロバスト性向上",
        "エラー訂正機構の追加",
        "ノイズ付きデータでファインチューニング"
    ]
    
else:
    ボトルネック = "全体的に良好"
    優先度 = "低"
    アクション = [
        "細かい改善で完成度向上",
        "ユーザーフィードバック収集"
    ]
```

---

## 📊 評価マトリックス

### 品質レベルの定義

| レベル | CER (ASR) | chrF++ (翻訳) | E2E chrF++ | 劣化度 | 総合評価 |
|--------|-----------|---------------|------------|--------|----------|
| **🏆 優秀** | < 5% | > 60 | > 55 | < 5 | 商用レベル |
| **✅ 良好** | 5-10% | 50-60 | 45-55 | 5-10 | **実用レベル（目標）** |
| **⚠️ 要改善** | 10-20% | 40-50 | 35-45 | 10-20 | 改善の余地 |
| **❌ 不合格** | > 20% | < 40 | < 35 | > 20 | 使えない |

### 評価結果の解釈マトリックス

```
         翻訳品質（chrF++）
           高(>50)    低(<50)
         ┌──────────┬──────────┐
ASR精度   │          │          │
高(<10%)  │    A     │    B     │
         │  最良    │ 翻訳改善  │
         ├──────────┼──────────┤
低(>10%)  │    C     │    D     │
         │ ASR改善  │ 両方改善  │
         └──────────┴──────────┘

A: 全体最適化、細かい改善
B: 翻訳モデル改善が最優先
C: ASRモデル改善が最優先
D: 両方改善が必要、優先順位を決定
```

---

## 🔄 改善サイクル

```
1. ベースライン測定
   ↓
   [現在のシステムの性能を記録]
   ↓
2. ボトルネック特定
   ↓
   [3段階評価 → 意思決定フロー]
   ↓
3. 改善実施
   ↓
   [優先度に基づいて改善作業]
   ↓
4. 再評価
   ↓
   [同じTED Talkで測定]
   ↓
5. 比較・検証
   ↓
   [改善効果の定量化]
   ↓
6. デプロイ判断
   ↓
   [目標達成？ → YES: デプロイ / NO: 戻る]
```

---

## 🛠️ 実装ステータス

### ✅ 完了

```
データ準備:
├── ✅ download_ted.py      - TED Talkダウンロード
├── ✅ vtt_to_text.py       - VTT→テキスト変換
└── ✅ データ収集           - 音声+韓国語字幕

評価スクリプト:
├── ✅ evaluate_asr.py         - ASR評価（CER/WER）
├── ✅ evaluate_translation.py - 翻訳評価（chrF++/BLEU）
├── ✅ evaluate_e2e.py         - E2E評価
└── ✅ quick_test.py           - クイックテスト

評価指標:
├── ✅ chrF++ (PRIMARY)      - 文字+単語 n-gram
├── ✅ BLEU (BASELINE)       - 業界標準
└── ⚠️ BERTScore (OPTIONAL)  - 環境問題で動作せず

ドキュメント:
├── ✅ README.md              - 全体ガイド
├── ✅ QUICK_START.md         - クイックスタート
├── ✅ metrics_research.md    - 学術的調査
├── ✅ metrics_analysis.md    - 指標詳細
└── ✅ EVALUATION_FRAMEWORK.md - このファイル
```

### ⏳ 保留中

```
データ準備:
└── ⏳ 日本語字幕           - YouTubeレート制限（18:00リトライ）

評価実行:
└── ⏳ 実データでの評価     - 日本語字幕取得後
```

---

## 📈 期待される結果（仮説）

### 予想スコア

```
Phase 1 (ASR):
  CER: 8-12%
  → 実用レベル、改善の余地あり

Phase 2 (翻訳・クリーン):
  chrF++: 48-55
  BLEU: 32-38
  → 実用レベル、目標付近

Phase 3 (E2E):
  chrF++: 42-50
  BLEU: 28-35
  → 劣化度: 5-8ポイント
  → ASRエラーの影響は中程度
```

### 予想される診断結果

```
if 実測値が予想通りなら:
    ボトルネック = "ASR精度の改善がやや有効"
    推奨アクション = [
        "ASRモデルの改善（優先度: 中）",
        "翻訳モデルの微調整（優先度: 中）"
    ]
```

---

## 🎯 評価実行計画

### ステップ1: 初回評価（18:00以降）

```bash
# 1. 日本語字幕取得
cd data/subtitles
yt-dlp --write-auto-sub --sub-lang ja ...

# 2. 翻訳評価
python3 evaluate_translation.py \
  data/subtitles/-WVhcG0rauI.ko.vtt \
  data/subtitles/-WVhcG0rauI.ja.vtt \
  -o data/results/baseline_translation.json

# 3. E2E評価
python3 evaluate_e2e.py \
  data/audio/-WVhcG0rauI.wav \
  data/subtitles/-WVhcG0rauI.ja.vtt \
  --reference-ko data/subtitles/-WVhcG0rauI.ko.vtt \
  -o data/results/baseline_e2e.json

# 4. 結果分析
cat data/results/baseline_*.json | jq
```

### ステップ2: 改善実施

```
[ボトルネック特定後に計画]
```

### ステップ3: 再評価と比較

```bash
# 同じTED Talkで再評価
python3 evaluate_translation.py ... -o improved_translation.json

# 改善効果の計算
diff baseline_translation.json improved_translation.json
```

---

## 🔬 品質保証

### 評価の信頼性を保つために

1. **複数データでテスト**
   - 最低3-5本のTED Talk
   - 平均スコアで判断

2. **統計的有意性**
   - 改善が5ポイント以上 → 有意
   - 改善が2ポイント以下 → 誤差範囲

3. **人間評価との相関**
   - 定期的にサンプルを人間が確認
   - chrF++と主観評価の一致を確認

4. **継続的モニタリング**
   - 週次で評価実行
   - スコアの推移を記録

---

## 📚 参考：評価指標の選定根拠

### なぜchrF++をPRIMARYに？

1. **人間評価との相関**: BLEU (r=0.51) < chrF++ (r=0.68) < COMET (r=0.84)
2. **日本語適性**: 文字レベル評価が適している
3. **計算速度**: リアルタイム評価可能
4. **実績**: WMT2023で軽量指標として最高評価

### なぜBLEUをBASELINEに？

1. **業界標準**: 30年の実績、比較しやすい
2. **シンプル**: 理解しやすい
3. **参考値**: chrF++と組み合わせて総合判断

### なぜBERTScoreはOPTIONAL？

1. **環境依存**: GPUやモデルの互換性問題
2. **計算コスト**: chrF++より遅い
3. **優先度**: chrF++で十分な精度

---

## 🎓 まとめ

### このフレームワークで何ができる？

✅ **問題の特定**
- ASRか翻訳か、どちらに問題があるか明確に

✅ **改善の定量化**
- 改善効果を数値で証明

✅ **優先順位の決定**
- 限られたリソースを最も効果的に投入

✅ **継続的改善**
- 評価→改善→再評価のサイクル

---

**最終更新:** 2026-02-08 14:28 JST
**ステータス:** フレームワーク完成、初回評価待機中（18:00）
