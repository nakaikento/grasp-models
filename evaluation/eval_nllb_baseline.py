#!/usr/bin/env python3
"""
NLLB ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ï¼ˆå­¦ç¿’å‰ï¼‰

AI Hub 100ã‚µãƒ³ãƒ—ãƒ«ã§äº‹å‰å­¦ç¿’æ¸ˆã¿NLLBã®ç¿»è¨³å“è³ªã‚’æ¸¬å®š
"""

import argparse
import time
from pathlib import Path
from tqdm import tqdm

import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import evaluate

# NLLBè¨€èªã‚³ãƒ¼ãƒ‰
LANG_CODES = {
    "ja": "jpn_Jpan",
    "ko": "kor_Hang",
}


def load_data(ko_file: Path, ja_file: Path, limit: int = 100):
    """AI Hubãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    with open(ko_file, 'r', encoding='utf-8') as f:
        ko_lines = [line.strip() for line in f][:limit]
    with open(ja_file, 'r', encoding='utf-8') as f:
        ja_lines = [line.strip() for line in f][:limit]
    return ko_lines, ja_lines


def translate_batch(model, tokenizer, texts, src_lang, tgt_lang, batch_size=8):
    """ãƒãƒƒãƒç¿»è¨³"""
    tokenizer.src_lang = LANG_CODES[src_lang]
    translations = []
    
    for i in tqdm(range(0, len(texts), batch_size), desc="Translating"):
        batch = texts[i:i + batch_size]
        
        inputs = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128,
        ).to(model.device)
        
        # å¼·åˆ¶çš„ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š
        forced_bos_token_id = tokenizer.convert_tokens_to_ids(LANG_CODES[tgt_lang])
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                forced_bos_token_id=forced_bos_token_id,
                max_new_tokens=128,
                num_beams=4,
            )
        
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        translations.extend(decoded)
    
    return translations


def evaluate_translations(hypotheses, references):
    """ç¿»è¨³å“è³ªã‚’è©•ä¾¡"""
    bleu = evaluate.load("sacrebleu")
    chrf = evaluate.load("chrf")
    
    bleu_result = bleu.compute(
        predictions=hypotheses,
        references=[[ref] for ref in references]
    )
    
    chrf_result = chrf.compute(
        predictions=hypotheses,
        references=[[ref] for ref in references],
        word_order=2,  # chrF++
    )
    
    return {
        "bleu": bleu_result["score"],
        "chrf": chrf_result["score"],
    }


def main():
    parser = argparse.ArgumentParser(description="NLLB ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡")
    parser.add_argument("--model", type=str, default="facebook/nllb-200-distilled-600M")
    parser.add_argument("--limit", type=int, default=100)
    parser.add_argument("--batch-size", type=int, default=8)
    parser.add_argument("--data-dir", type=str, default="evaluation/data/aihub")
    args = parser.parse_args()
    
    print("=" * 60)
    print(f"NLLB ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡: ko â†’ ja")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {args.limit}")
    print()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(args.data_dir)
    ko_lines, ja_refs = load_data(
        data_dir / "ko_reference.txt",
        data_dir / "ja_source.txt",
        limit=args.limit,
    )
    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(ko_lines)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print(f"\nãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {args.model}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForSeq2SeqLM.from_pretrained(args.model).to(device)
    model.eval()
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:,} ({num_params/1e6:.0f}M)")
    
    # ç¿»è¨³
    print(f"\nç¿»è¨³ä¸­...")
    start_time = time.time()
    
    ja_hyps = translate_batch(
        model, tokenizer, ko_lines,
        src_lang="ko", tgt_lang="ja",
        batch_size=args.batch_size,
    )
    
    elapsed = time.time() - start_time
    print(f"ç¿»è¨³å®Œäº†: {elapsed:.1f}ç§’ ({len(ko_lines)/elapsed:.1f} samples/s)")
    
    # è©•ä¾¡
    print(f"\nè©•ä¾¡ä¸­...")
    results = evaluate_translations(ja_hyps, ja_refs)
    
    print()
    print("=" * 60)
    print("ğŸ“Š çµæœ")
    print("=" * 60)
    print(f"BLEU:   {results['bleu']:.2f}")
    print(f"chrF++: {results['chrf']:.2f}")
    print()
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print("=" * 60)
    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ç¿»è¨³")
    print("=" * 60)
    for i in [0, 1, 2, 49, 99]:
        if i < len(ko_lines):
            print(f"\n[{i+1}]")
            print(f"KO:  {ko_lines[i]}")
            print(f"REF: {ja_refs[i]}")
            print(f"HYP: {ja_hyps[i]}")


if __name__ == "__main__":
    main()
