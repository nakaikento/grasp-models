#!/usr/bin/env python3
"""
Qwen2.5-72B-Instruct-AWQ ç¿»è¨³å“è³ªè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
    python eval_qwen72b_awq.py --samples 100

å¿…è¦ç’°å¢ƒ:
    - NVIDIA GPU (RTX 6000 Blackwell 97GBæ¨å¥¨)
    - vLLM 0.6.0+
    - sacrebleu
"""

import argparse
import json
import time
from pathlib import Path

try:
    from vllm import LLM, SamplingParams
except ImportError:
    print("âŒ vLLMãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("   pip install vllm")
    exit(1)

try:
    import sacrebleu
except ImportError:
    print("âŒ sacrebleuãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("   pip install sacrebleu")
    exit(1)


# === è¨­å®š ===
MODEL_ID = "Qwen/Qwen2.5-72B-Instruct-AWQ"
DATA_FILE = Path(__file__).parent / "data" / "ko_ja_100.jsonl"
OUTPUT_FILE = Path(__file__).parent / "results" / "qwen72b_awq_results.json"

SYSTEM_PROMPT = """ã‚ãªãŸã¯éŸ“å›½èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³è€…ã§ã™ã€‚
å…¥åŠ›ã•ã‚ŒãŸéŸ“å›½èªã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
ç¿»è¨³ã®ã¿ã‚’å‡ºåŠ›ã—ã€èª¬æ˜ã‚„è£œè¶³ã¯ä¸€åˆ‡åŠ ãˆãªã„ã§ãã ã•ã„ã€‚
é€šè²¨ã‚„å˜ä½ã¯å¤‰æ›ã›ãšã€ãã®ã¾ã¾ç¶­æŒã—ã¦ãã ã•ã„ã€‚
æ—¥æœ¬èªã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä¸­å›½èªã‚’æ··ãœãªã„ã§ãã ã•ã„ï¼‰ã€‚"""


def load_data(filepath: Path, max_samples: int = 100) -> list[dict]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    data = []
    with open(filepath, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= max_samples:
                break
            item = json.loads(line.strip())
            data.append({
                "ko": item["ko"],
                "ja_ref": item["ja"],
            })
    return data


def translate_batch(llm: LLM, texts: list[str], sampling_params: SamplingParams) -> list[str]:
    """ãƒãƒƒãƒç¿»è¨³"""
    prompts = []
    for text in texts:
        prompt = f"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n{text}<|im_end|>\n<|im_start|>assistant\n"
        prompts.append(prompt)
    
    outputs = llm.generate(prompts, sampling_params)
    results = []
    for output in outputs:
        generated = output.outputs[0].text.strip()
        results.append(generated)
    return results


def calculate_metrics(hypotheses: list[str], references: list[str]) -> dict:
    """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    # chrF++
    chrf = sacrebleu.corpus_chrf(hypotheses, [references], word_order=2)
    
    # BLEU
    bleu = sacrebleu.corpus_bleu(hypotheses, [references])
    
    return {
        "chrf++": round(chrf.score, 2),
        "bleu": round(bleu.score, 2),
        "num_samples": len(hypotheses),
    }


def main():
    parser = argparse.ArgumentParser(description="Qwen2.5-72B-AWQ Koâ†’Jaç¿»è¨³è©•ä¾¡")
    parser.add_argument("--samples", type=int, default=100, help="è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°")
    parser.add_argument("--batch-size", type=int, default=10, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument("--model", type=str, default=MODEL_ID, help="ãƒ¢ãƒ‡ãƒ«ID")
    parser.add_argument("--data", type=str, default=None, help="ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    args = parser.parse_args()

    data_file = Path(args.data) if args.data else DATA_FILE
    
    print("=" * 60)
    print(f"Qwen2.5-72B-AWQ ç¿»è¨³å“è³ªè©•ä¾¡")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {args.samples}")
    print(f"ãƒ‡ãƒ¼ã‚¿: {data_file}")
    print()

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    if not data_file.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_file}")
        print("   å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ãã ã•ã„:")
        print("   python prepare_eval_data.py")
        return
    
    data = load_data(data_file, args.samples)
    print(f"   {len(data)} ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    print()

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    start_load = time.time()
    llm = LLM(
        model=args.model,
        quantization="awq",
        dtype="float16",
        gpu_memory_utilization=0.9,
        max_model_len=2048,
        enforce_eager=True,  # Skip torch.compile to avoid disk space issues
    )
    load_time = time.time() - start_load
    print(f"   ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.1f}ç§’)")
    print()

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    sampling_params = SamplingParams(
        temperature=0.0,  # Greedy decoding
        max_tokens=256,
        stop=["<|im_end|>", "\n\n"],
    )

    # ç¿»è¨³å®Ÿè¡Œ
    print("ğŸ”„ ç¿»è¨³ä¸­...")
    ko_texts = [d["ko"] for d in data]
    start_translate = time.time()
    
    hypotheses = translate_batch(llm, ko_texts, sampling_params)
    
    translate_time = time.time() - start_translate
    speed = len(data) / translate_time
    print(f"   ç¿»è¨³å®Œäº†: {translate_time:.1f}ç§’ ({speed:.2f} samples/s)")
    print()

    # è©•ä¾¡
    print("ğŸ“Š è©•ä¾¡ä¸­...")
    references = [d["ja_ref"] for d in data]
    metrics = calculate_metrics(hypotheses, references)
    print(f"   chrF++: {metrics['chrf++']}")
    print(f"   BLEU:   {metrics['bleu']}")
    print()

    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ç¿»è¨³ (æœ€åˆã®5ä»¶):")
    print("-" * 60)
    for i in range(min(5, len(data))):
        print(f"[{i+1}]")
        print(f"  KO:  {data[i]['ko']}")
        print(f"  REF: {data[i]['ja_ref']}")
        print(f"  HYP: {hypotheses[i]}")
        print()

    # çµæœä¿å­˜
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    results = {
        "model": args.model,
        "num_samples": len(data),
        "metrics": metrics,
        "timing": {
            "load_time_sec": round(load_time, 1),
            "translate_time_sec": round(translate_time, 1),
            "speed_samples_per_sec": round(speed, 2),
        },
        "representative_samples": [
            {
                "type": label,
                "ko": data[idx]["ko"],
                "ja_ref": data[idx]["ja_ref"],
                "ja_hyp": hypotheses[idx],
            }
            for label, idx in zip(["short", "medium", "long"], representative)
        ],
        "translations": [
            {
                "ko": data[i]["ko"],
                "ja_ref": data[i]["ja_ref"],
                "ja_hyp": hypotheses[i],
            }
            for i in range(len(data))
        ],
    }
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ çµæœä¿å­˜: {OUTPUT_FILE}")

    # ä»£è¡¨ã‚µãƒ³ãƒ—ãƒ«3æ–‡ã‚’é¸æŠï¼ˆçŸ­æ–‡ãƒ»ä¸­æ–‡ãƒ»é•·æ–‡ï¼‰
    samples_by_len = sorted(enumerate(data), key=lambda x: len(x[1]["ko"]))
    representative = [
        samples_by_len[len(samples_by_len) // 4][0],      # çŸ­ã‚
        samples_by_len[len(samples_by_len) // 2][0],      # ä¸­é–“
        samples_by_len[3 * len(samples_by_len) // 4][0],  # é•·ã‚
    ]

    # ã‚µãƒãƒªãƒ¼
    print()
    print("=" * 60)
    print("ğŸ“Š ã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«:     {args.model}")
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(data)}")
    print(f"chrF++:     {metrics['chrf++']}")
    print(f"BLEU:       {metrics['bleu']}")
    print(f"å‡¦ç†æ™‚é–“:   {translate_time:.1f}ç§’")
    print(f"é€Ÿåº¦:       {speed:.2f} samples/s")
    print()
    print("ğŸ“ ä»£è¡¨ã‚µãƒ³ãƒ—ãƒ« (çŸ­/ä¸­/é•·):")
    print("-" * 60)
    for idx in representative:
        print(f"KO:  {data[idx]['ko']}")
        print(f"REF: {data[idx]['ja_ref']}")
        print(f"HYP: {hypotheses[idx]}")
        print()
    print("=" * 60)


if __name__ == "__main__":
    main()
