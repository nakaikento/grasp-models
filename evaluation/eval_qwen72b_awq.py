#!/usr/bin/env python3
"""
Qwen2.5-72B-Instruct-AWQ ç¿»è¨³å“è³ªè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
    python eval_qwen72b_awq.py --samples 100

å¿…è¦ç’°å¢ƒ:
    - NVIDIA GPU (RTX 6000 Blackwell 97GBæ¨å¥¨)
    - vLLM 0.6.0+
    - sacrebleu
"""

import argparse
import json
import time
from pathlib import Path

try:
    from vllm import LLM, SamplingParams
except ImportError:
    print("âŒ vLLMãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("   pip install vllm")
    exit(1)

try:
    import sacrebleu
except ImportError:
    print("âŒ sacrebleuãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("   pip install sacrebleu")
    exit(1)


# === è¨­å®š ===
MODEL_ID = "Qwen/Qwen2.5-72B-Instruct-AWQ"
DATA_FILE = Path(__file__).parent / "data" / "aihub"  # AI Hub directory
OUTPUT_FILE = Path(__file__).parent / "results" / "qwen72b_awq_results.json"

SYSTEM_PROMPT = """ã‚ãªãŸã¯éŸ“å›½ãƒ‰ãƒ©ãƒãƒ»æ˜ ç”»ãƒ»ã‚¢ãƒ‹ãƒ¡ã®å­—å¹•ç¿»è¨³ã‚’å°‚é–€ã¨ã™ã‚‹ç¿»è¨³è€…ã§ã™ã€‚
è¦–è´è€…ãŒç”»é¢ã‚’è¦‹ãªãŒã‚‰è‡ªç„¶ã«ç†è§£ã§ãã‚‹å­—å¹•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ç¿»è¨³æ–¹é‡ã€‘
- éŸ“å›½èªã®æ„å‘³ã¨ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’æ­£ç¢ºã«ä¼ãˆã‚‹è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³
- æ–‡åŒ–çš„ãªèƒŒæ™¯ã‚’è€ƒæ…®ã—ã€æ—¥æœ¬äººè¦–è´è€…ã«é•å’Œæ„Ÿãªãä¼ã‚ã‚‹è¡¨ç¾ã‚’ä½¿ç”¨
- æ•¬èªãƒ»ã‚¿ãƒ¡å£ã®ãƒ¬ãƒ™ãƒ«ã¯åŸæ–‡ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ã«åˆã‚ã›ã‚‹
- å­—å¹•ã¨ã—ã¦èª­ã¿ã‚„ã™ã„ç°¡æ½”ãªè¡¨ç¾ã‚’å¿ƒãŒã‘ã‚‹

ã€å³å®ˆäº‹é …ã€‘
- ç¿»è¨³æ–‡ã®ã¿ã‚’å‡ºåŠ›ï¼ˆèª¬æ˜ãƒ»è£œè¶³ã¯ä¸è¦ï¼‰
- é€šè²¨ãƒ»å˜ä½ã¯ãã®ã¾ã¾ç¶­æŒï¼ˆã‚¦ã‚©ãƒ³â†’å††ã¸ã®å¤‰æ›ç¦æ­¢ï¼‰
- å›ºæœ‰åè©ã¯åŸéŸ³ã«è¿‘ã„ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜
- æ—¥æœ¬èªã®ã¿ã§å‡ºåŠ›ï¼ˆä¸­å›½èªæ··å…¥ç¦æ­¢ï¼‰

ã€ç¿»è¨³ä¾‹ã€‘
éŸ“: ê²½ê¸° ë‹¹ì¼ì—ëŠ” ë‚ ì”¨ê°€ ì¢‹ë„¤.
æ—¥: ç«¶æŠ€å½“æ—¥ã«ã¯å¤©æ°—ãŒã„ã„ã­ã€‚

éŸ“: ë‹¹ì‹ ì€ ë¬´ìŠ¨ ë§› ì•„ì´ìŠ¤í¬ë¦¼ì„ ì›í•˜ë‚˜ìš”?
æ—¥: ã‚ãªãŸã¯ä½•å‘³ã®ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ãŒã»ã—ã„ã§ã™ã‹ï¼Ÿ

éŸ“: ê³ ê°ë‹˜ ì±…ì œëª©ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ë°”ë¡œ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
æ—¥: ãŠå®¢æ§˜ã€æœ¬ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãŠã£ã—ã‚ƒã£ã¦ã„ãŸã ã‘ã‚Œã°ã€ã™ãã«ã”æ¡ˆå†…ã—ã¾ã™ã€‚"""


def load_data(filepath: Path, max_samples: int = 100) -> list[dict]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆjsonl or txt pairå¯¾å¿œï¼‰"""
    data = []
    
    # Check if it's a directory with txt files (AI Hub format)
    if filepath.is_dir():
        ko_file = filepath / "ko_reference.txt"
        ja_file = filepath / "ja_source.txt"
        if ko_file.exists() and ja_file.exists():
            with open(ko_file, "r", encoding="utf-8") as f_ko, \
                 open(ja_file, "r", encoding="utf-8") as f_ja:
                for i, (ko_line, ja_line) in enumerate(zip(f_ko, f_ja)):
                    if i >= max_samples:
                        break
                    data.append({
                        "ko": ko_line.strip(),
                        "ja_ref": ja_line.strip(),
                    })
            return data
    
    # jsonl format
    with open(filepath, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= max_samples:
                break
            item = json.loads(line.strip())
            data.append({
                "ko": item["ko"],
                "ja_ref": item["ja"],
            })
    return data


def translate_batch(llm: LLM, texts: list[str], sampling_params: SamplingParams) -> list[str]:
    """ãƒãƒƒãƒç¿»è¨³"""
    prompts = []
    for text in texts:
        prompt = f"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n{text}<|im_end|>\n<|im_start|>assistant\n"
        prompts.append(prompt)
    
    outputs = llm.generate(prompts, sampling_params)
    results = []
    for output in outputs:
        generated = output.outputs[0].text.strip()
        results.append(generated)
    return results


def calculate_metrics(hypotheses: list[str], references: list[str]) -> dict:
    """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    # chrF++
    chrf = sacrebleu.corpus_chrf(hypotheses, [references], word_order=2)
    
    # BLEU
    bleu = sacrebleu.corpus_bleu(hypotheses, [references])
    
    return {
        "chrf++": round(chrf.score, 2),
        "bleu": round(bleu.score, 2),
        "num_samples": len(hypotheses),
    }


def main():
    parser = argparse.ArgumentParser(description="Qwen2.5-72B-AWQ Koâ†’Jaç¿»è¨³è©•ä¾¡")
    parser.add_argument("--samples", type=int, default=100, help="è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°")
    parser.add_argument("--batch-size", type=int, default=10, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument("--model", type=str, default=MODEL_ID, help="ãƒ¢ãƒ‡ãƒ«ID")
    parser.add_argument("--data", type=str, default=None, help="ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    args = parser.parse_args()

    data_file = Path(args.data) if args.data else DATA_FILE
    
    print("=" * 60)
    print(f"Qwen2.5-72B-AWQ ç¿»è¨³å“è³ªè©•ä¾¡")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {args.samples}")
    print(f"ãƒ‡ãƒ¼ã‚¿: {data_file}")
    print()

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    if not data_file.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_file}")
        print("   å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ãã ã•ã„:")
        print("   python prepare_eval_data.py")
        return
    
    data = load_data(data_file, args.samples)
    print(f"   {len(data)} ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    print()

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    start_load = time.time()
    llm = LLM(
        model=args.model,
        quantization="awq",
        dtype="float16",
        gpu_memory_utilization=0.9,
        max_model_len=2048,
        enforce_eager=True,  # Skip torch.compile to avoid disk space issues
    )
    load_time = time.time() - start_load
    print(f"   ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.1f}ç§’)")
    print()

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    sampling_params = SamplingParams(
        temperature=0.0,  # Greedy decoding
        max_tokens=256,
        stop=["<|im_end|>", "\n\n"],
    )

    # ç¿»è¨³å®Ÿè¡Œ
    print("ğŸ”„ ç¿»è¨³ä¸­...")
    ko_texts = [d["ko"] for d in data]
    start_translate = time.time()
    
    hypotheses = translate_batch(llm, ko_texts, sampling_params)
    
    translate_time = time.time() - start_translate
    speed = len(data) / translate_time
    print(f"   ç¿»è¨³å®Œäº†: {translate_time:.1f}ç§’ ({speed:.2f} samples/s)")
    print()

    # è©•ä¾¡
    print("ğŸ“Š è©•ä¾¡ä¸­...")
    references = [d["ja_ref"] for d in data]
    metrics = calculate_metrics(hypotheses, references)
    print(f"   chrF++: {metrics['chrf++']}")
    print(f"   BLEU:   {metrics['bleu']}")
    print()

    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ç¿»è¨³ (æœ€åˆã®5ä»¶):")
    print("-" * 60)
    for i in range(min(5, len(data))):
        print(f"[{i+1}]")
        print(f"  KO:  {data[i]['ko']}")
        print(f"  REF: {data[i]['ja_ref']}")
        print(f"  HYP: {hypotheses[i]}")
        print()

    # ä»£è¡¨ã‚µãƒ³ãƒ—ãƒ«3æ–‡ã‚’é¸æŠï¼ˆçŸ­æ–‡ãƒ»ä¸­æ–‡ãƒ»é•·æ–‡ï¼‰
    samples_by_len = sorted(enumerate(data), key=lambda x: len(x[1]["ko"]))
    representative = [
        samples_by_len[len(samples_by_len) // 4][0],      # çŸ­ã‚
        samples_by_len[len(samples_by_len) // 2][0],      # ä¸­é–“
        samples_by_len[3 * len(samples_by_len) // 4][0],  # é•·ã‚
    ]

    # çµæœä¿å­˜
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    results = {
        "model": args.model,
        "num_samples": len(data),
        "metrics": metrics,
        "timing": {
            "load_time_sec": round(load_time, 1),
            "translate_time_sec": round(translate_time, 1),
            "speed_samples_per_sec": round(speed, 2),
        },
        "representative_samples": [
            {
                "type": label,
                "ko": data[idx]["ko"],
                "ja_ref": data[idx]["ja_ref"],
                "ja_hyp": hypotheses[idx],
            }
            for label, idx in zip(["short", "medium", "long"], representative)
        ],
        "translations": [
            {
                "ko": data[i]["ko"],
                "ja_ref": data[i]["ja_ref"],
                "ja_hyp": hypotheses[i],
            }
            for i in range(len(data))
        ],
    }
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ çµæœä¿å­˜: {OUTPUT_FILE}")

    # ã‚µãƒãƒªãƒ¼
    print()
    print("=" * 60)
    print("ğŸ“Š ã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«:     {args.model}")
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(data)}")
    print(f"chrF++:     {metrics['chrf++']}")
    print(f"BLEU:       {metrics['bleu']}")
    print(f"å‡¦ç†æ™‚é–“:   {translate_time:.1f}ç§’")
    print(f"é€Ÿåº¦:       {speed:.2f} samples/s")
    print()
    print("ğŸ“ ä»£è¡¨ã‚µãƒ³ãƒ—ãƒ« (çŸ­/ä¸­/é•·):")
    print("-" * 60)
    for idx in representative:
        print(f"KO:  {data[idx]['ko']}")
        print(f"REF: {data[idx]['ja_ref']}")
        print(f"HYP: {hypotheses[idx]}")
        print()
    print("=" * 60)


if __name__ == "__main__":
    main()
