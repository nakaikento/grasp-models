#!/usr/bin/env python3
"""
è¤‡æ•°ã®LLMç¿»è¨³çµæœã‚’æ¯”è¼ƒè©•ä¾¡ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

ä½¿ç”¨æ–¹æ³•:
  python compare_translations.py \
    --source samples/source_ko.txt \
    --reference samples/reference_ja.txt \
    --translations translations/qwen3-32b-natural.txt translations/deepseek-r1.txt \
    --names "Qwen3-32B" "DeepSeek-R1" \
    --output results/comparison.json
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, asdict
from statistics import mean, stdev

@dataclass
class Metrics:
    chrf_pp: Optional[float] = None
    bleu: Optional[float] = None
    bertscore_f1: Optional[float] = None
    comet: Optional[float] = None

def calculate_metrics_single(source: str, reference: str, hypothesis: str) -> Metrics:
    """å˜ä¸€ã®ç¿»è¨³ãƒšã‚¢ã«å¯¾ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
    metrics = Metrics()
    
    # chrF++
    try:
        from sacrebleu import sentence_chrf
        result = sentence_chrf(hypothesis, [reference], word_order=2)
        metrics.chrf_pp = round(result.score, 2)
    except Exception as e:
        print(f"âš ï¸ chrF++ error: {e}")
    
    # BLEU
    try:
        from sacrebleu import sentence_bleu
        result = sentence_bleu(hypothesis, [reference], tokenize='char')
        metrics.bleu = round(result.score, 2)
    except Exception as e:
        print(f"âš ï¸ BLEU error: {e}")
    
    return metrics

def calculate_corpus_metrics(sources: list[str], references: list[str], hypotheses: list[str]) -> dict:
    """ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã«å¯¾ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
    metrics = {}
    
    # chrF++ (corpus-level)
    try:
        from sacrebleu import corpus_chrf
        result = corpus_chrf(hypotheses, [references], word_order=2)
        metrics['chrf_pp'] = round(result.score, 2)
    except Exception as e:
        print(f"âš ï¸ corpus chrF++ error: {e}")
    
    # BLEU (corpus-level)
    try:
        from sacrebleu import corpus_bleu
        result = corpus_bleu(hypotheses, [references], tokenize='char')
        metrics['bleu'] = round(result.score, 2)
    except Exception as e:
        print(f"âš ï¸ corpus BLEU error: {e}")
    
    # COMET (optional, requires GPU)
    try:
        from comet import load_from_checkpoint, download_model
        
        model_path = download_model("Unbabel/wmt22-comet-da")
        model = load_from_checkpoint(model_path)
        
        data = [
            {"src": s, "mt": h, "ref": r}
            for s, h, r in zip(sources, hypotheses, references)
        ]
        
        output = model.predict(data, batch_size=8, gpus=1)
        metrics['comet'] = round(output['system_score'], 4)
        
    except ImportError:
        pass  # COMET is optional
    except Exception as e:
        print(f"âš ï¸ COMET error: {e}")
    
    return metrics

def load_lines(path: Path) -> list[str]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¡Œã‚’èª­ã¿è¾¼ã‚€"""
    with open(path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f]

def print_comparison_table(results: list[dict]):
    """æ¯”è¼ƒçµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤º"""
    print("\n" + "="*70)
    print("ğŸ“Š TRANSLATION QUALITY COMPARISON")
    print("="*70)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    header = f"{'Model':<25} {'chrF++':>10} {'BLEU':>10}"
    if any(r.get('comet') for r in results):
        header += f" {'COMET':>10}"
    print(header)
    print("-"*70)
    
    # ã‚½ãƒ¼ãƒˆï¼ˆchrF++é™é †ï¼‰
    sorted_results = sorted(results, key=lambda x: x.get('chrf_pp', 0), reverse=True)
    
    for i, r in enumerate(sorted_results):
        rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
        row = f"{rank} {r['name']:<22} {r.get('chrf_pp', 'N/A'):>10}"
        row += f" {r.get('bleu', 'N/A'):>10}"
        if any(x.get('comet') for x in results):
            row += f" {r.get('comet', 'N/A'):>10}"
        print(row)
    
    print("="*70)
    
    # åˆ†æ
    print("\nğŸ” ANALYSIS")
    best = sorted_results[0]
    print(f"   Best model: {best['name']} (chrF++ {best.get('chrf_pp', 'N/A')})")
    
    chrf_scores = [r['chrf_pp'] for r in results if r.get('chrf_pp')]
    if len(chrf_scores) > 1:
        print(f"   chrF++ range: {min(chrf_scores):.1f} - {max(chrf_scores):.1f} (Î”{max(chrf_scores)-min(chrf_scores):.1f})")
    
    # å“è³ªåˆ¤å®š
    if best.get('chrf_pp', 0) >= 50:
        print("   âœ… Top model meets production quality threshold (chrF++ â‰¥ 50)")
    else:
        print("   âš ï¸ No model meets production quality threshold (chrF++ < 50)")

def main():
    parser = argparse.ArgumentParser(description="LLMç¿»è¨³æ¯”è¼ƒè©•ä¾¡")
    parser.add_argument("--source", type=Path, required=True, help="ã‚½ãƒ¼ã‚¹éŸ“å›½èªãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--reference", type=Path, required=True, help="ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--translations", type=Path, nargs='+', required=True, help="ç¿»è¨³çµæœãƒ•ã‚¡ã‚¤ãƒ«ç¾¤")
    parser.add_argument("--names", type=str, nargs='+', help="ãƒ¢ãƒ‡ãƒ«åï¼ˆç¿»è¨³ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒæ•°ï¼‰")
    parser.add_argument("--output", type=Path, help="çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (JSON)")
    args = parser.parse_args()
    
    # èª­ã¿è¾¼ã¿
    sources = load_lines(args.source)
    references = load_lines(args.reference)
    
    print(f"ğŸ“¥ Source: {len(sources)} lines")
    print(f"ğŸ“¥ Reference: {len(references)} lines")
    
    # å„ç¿»è¨³çµæœã‚’è©•ä¾¡
    all_results = []
    
    for i, trans_path in enumerate(args.translations):
        name = args.names[i] if args.names and i < len(args.names) else trans_path.stem
        print(f"\nğŸ”„ Evaluating: {name}")
        
        hypotheses = load_lines(trans_path)
        
        # è¡Œæ•°ãƒã‚§ãƒƒã‚¯
        min_len = min(len(sources), len(references), len(hypotheses))
        if min_len < len(sources):
            print(f"   âš ï¸ Truncating to {min_len} lines")
        
        src = sources[:min_len]
        ref = references[:min_len]
        hyp = hypotheses[:min_len]
        
        # ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ¬ãƒ™ãƒ«è©•ä¾¡
        metrics = calculate_corpus_metrics(src, ref, hyp)
        metrics['name'] = name
        metrics['file'] = str(trans_path)
        metrics['n_samples'] = min_len
        
        all_results.append(metrics)
        print(f"   chrF++: {metrics.get('chrf_pp', 'N/A')}, BLEU: {metrics.get('bleu', 'N/A')}")
    
    # æ¯”è¼ƒè¡¨ç¤º
    print_comparison_table(all_results)
    
    # çµæœä¿å­˜
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump({
                'source_file': str(args.source),
                'reference_file': str(args.reference),
                'n_samples': len(sources),
                'results': all_results
            }, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… Results saved: {args.output}")

if __name__ == "__main__":
    main()
