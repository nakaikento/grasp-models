#!/usr/bin/env python3
"""
AI Hub Ko-Ja Translation ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºã€‚

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: traintogpb/aihub-koja-translation-integrated-small-100k
ãƒ•ã‚£ãƒ«ã‚¿:
  - aihub-71263: æ”¾é€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆãƒ‰ãƒ©ãƒå­—å¹•ï¼‰
  - aihub-546: æ—¥å¸¸ç”Ÿæ´»ãƒ»å£èª

ä½¿ç”¨æ–¹æ³•:
  python3 extract_aihub.py --n-samples 10000 --seed 71263
"""

import argparse
import random
from pathlib import Path
from datasets import load_dataset

# æ˜ ç”»ãƒ»ãƒ‰ãƒ©ãƒãƒ»ã‚¢ãƒ‹ãƒ¡å­¦ç¿’ã«é©ã—ãŸã‚½ãƒ¼ã‚¹
TARGET_SOURCES = {'aihub-71263', 'aihub-546'}

def main():
    parser = argparse.ArgumentParser(description="AI Hubã‹ã‚‰æ—¥éŸ“ãƒšã‚¢æŠ½å‡º")
    parser.add_argument("--output-dir", type=Path, default=Path("data/aihub"))
    parser.add_argument("--n-samples", type=int, default=10000)
    parser.add_argument("--seed", type=int, default=71263, help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰")
    parser.add_argument("--min-len", type=int, default=5, help="æœ€å°æ–‡å­—æ•°")
    parser.add_argument("--max-len", type=int, default=200, help="æœ€å¤§æ–‡å­—æ•°")
    parser.add_argument("--all-sources", action="store_true", help="å…¨ã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨")
    args = parser.parse_args()
    
    print("ğŸ“¥ Loading AI Hub Ko-Ja Translation dataset...")
    ds = load_dataset(
        'traintogpb/aihub-koja-translation-integrated-small-100k', 
        split='train'
    )
    print(f"   Total samples: {len(ds)}")
    
    # ã‚½ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
    if not args.all_sources:
        print(f"ğŸ¯ Filtering by source: {TARGET_SOURCES}")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    print(f"ğŸ” Filtering (length: {args.min_len}-{args.max_len})...")
    filtered = []
    source_counts = {}
    
    for item in ds:
        source = item['source']
        
        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
        if not args.all_sources and source not in TARGET_SOURCES:
            continue
        
        ja = item['ja'].strip()
        ko = item['ko'].strip()
        
        # é•·ã•ãƒã‚§ãƒƒã‚¯
        if not (args.min_len <= len(ja) <= args.max_len):
            continue
        if not (args.min_len <= len(ko) <= args.max_len):
            continue
        
        # ç©ºç™½ãƒã‚§ãƒƒã‚¯
        if not ja or not ko:
            continue
        
        filtered.append((ja, ko, source))
        source_counts[source] = source_counts.get(source, 0) + 1
    
    print(f"   Filtered: {len(filtered)} samples")
    for src, cnt in sorted(source_counts.items()):
        print(f"     {src}: {cnt}")
    
    # ã‚½ãƒ¼ã‚¹æ¯”ç‡ã‚’ç¶­æŒã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    random.seed(args.seed)
    
    if len(filtered) <= args.n_samples:
        samples = filtered
    else:
        # ã‚½ãƒ¼ã‚¹ã”ã¨ã«æ¯”ç‡ã‚’è¨ˆç®—
        total = len(filtered)
        samples = []
        
        for source in source_counts:
            source_items = [(ja, ko) for ja, ko, src in filtered if src == source]
            n_take = int(args.n_samples * (len(source_items) / total))
            n_take = max(1, n_take)  # æœ€ä½1ã¤ã¯å–ã‚‹
            
            if len(source_items) <= n_take:
                samples.extend(source_items)
            else:
                samples.extend(random.sample(source_items, n_take))
        
        # ä¸è¶³åˆ†ã‚’è£œå……
        if len(samples) < args.n_samples:
            all_pairs = [(ja, ko) for ja, ko, _ in filtered]
            remaining = [p for p in all_pairs if p not in samples]
            random.shuffle(remaining)
            samples.extend(remaining[:args.n_samples - len(samples)])
        
        random.shuffle(samples)
        samples = samples[:args.n_samples]
    
    print(f"\nğŸ“Š Selected {len(samples)} samples")
    
    # ä¿å­˜
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    ja_file = args.output_dir / "ja_source.txt"
    ko_file = args.output_dir / "ko_reference.txt"
    
    with open(ja_file, 'w', encoding='utf-8') as f:
        for ja, ko in samples:
            f.write(ja + '\n')
    
    with open(ko_file, 'w', encoding='utf-8') as f:
        for ja, ko in samples:
            f.write(ko + '\n')
    
    # çµ±è¨ˆ
    ja_lengths = [len(ja) for ja, _ in samples]
    ko_lengths = [len(ko) for _, ko in samples]
    
    stats_file = args.output_dir / "stats.txt"
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write(f"Dataset: AI Hub Ko-Ja Translation\n")
        f.write(f"Sources: {', '.join(sorted(source_counts.keys()))}\n")
        f.write(f"Seed: {args.seed}\n")
        f.write(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(samples)}\n")
        f.write(f"æ—¥æœ¬èªå¹³å‡é•·: {sum(ja_lengths)/len(ja_lengths):.1f}æ–‡å­—\n")
        f.write(f"æ—¥æœ¬èªæœ€å°/æœ€å¤§: {min(ja_lengths)}/{max(ja_lengths)}æ–‡å­—\n")
        f.write(f"éŸ“å›½èªå¹³å‡é•·: {sum(ko_lengths)/len(ko_lengths):.1f}æ–‡å­—\n")
        f.write(f"éŸ“å›½èªæœ€å°/æœ€å¤§: {min(ko_lengths)}/{max(ko_lengths)}æ–‡å­—\n")
    
    print(f"\nâœ… Saved to {args.output_dir}")
    print(f"   - {ja_file.name}: {len(samples)} lines")
    print(f"   - {ko_file.name}: {len(samples)} lines")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print(f"\nğŸ“ Sample (first 5):")
    for i in range(min(5, len(samples))):
        ja, ko = samples[i]
        print(f"   [{i}] JA: {ja[:50]}...")
        print(f"       KO: {ko[:50]}...")

if __name__ == "__main__":
    main()
