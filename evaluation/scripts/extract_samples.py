#!/usr/bin/env python3
"""
OpenSubtitlesã‹ã‚‰è©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
ã‚¢ãƒ‹ãƒ¡/ãƒ‰ãƒ©ãƒã®ä¼šè©±æ–‡ã‚’æƒ³å®šã—ã€é©åˆ‡ãªé•·ã•ã¨å¤šæ§˜æ€§ã‚’æŒã¤ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸å®šã€‚
"""

import random
import argparse
from pathlib import Path
from collections import Counter

def load_parallel_data(ko_file: Path, ja_file: Path) -> list[tuple[str, str]]:
    """ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    with open(ko_file, 'r', encoding='utf-8') as f:
        ko_lines = [line.strip() for line in f]
    with open(ja_file, 'r', encoding='utf-8') as f:
        ja_lines = [line.strip() for line in f]
    
    assert len(ko_lines) == len(ja_lines), "è¡Œæ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“"
    return list(zip(ko_lines, ja_lines))

def filter_good_samples(pairs: list[tuple[str, str]], 
                        min_len: int = 10, 
                        max_len: int = 100) -> list[tuple[str, str]]:
    """å“è³ªã®é«˜ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = []
    
    for ko, ja in pairs:
        # é•·ã•ãƒã‚§ãƒƒã‚¯
        if not (min_len <= len(ko) <= max_len and min_len <= len(ja) <= max_len):
            continue
        
        # ç©ºç™½ã‚„ç‰¹æ®Šæ–‡å­—ã ã‘ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’é™¤å¤–
        if not ko.strip() or not ja.strip():
            continue
        
        # æ•°å­—ã ã‘ã€è¨˜å·ã ã‘ã‚’é™¤å¤–
        if ko.isdigit() or ja.isdigit():
            continue
        
        # ãƒãƒ³ã‚°ãƒ«/æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        has_hangul = any('\uac00' <= c <= '\ud7a3' for c in ko)
        has_jp = any(('\u3040' <= c <= '\u309f') or  # ã²ã‚‰ãŒãª
                     ('\u30a0' <= c <= '\u30ff') or  # ã‚«ã‚¿ã‚«ãƒŠ
                     ('\u4e00' <= c <= '\u9fff')     # æ¼¢å­—
                     for c in ja)
        
        if not has_hangul or not has_jp:
            continue
        
        filtered.append((ko, ja))
    
    return filtered

def select_diverse_samples(pairs: list[tuple[str, str]], 
                           n_samples: int = 1000,
                           seed: int = 42) -> list[tuple[str, str]]:
    """å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸å®š"""
    random.seed(seed)
    
    # é•·ã•ã§å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    short = [(ko, ja) for ko, ja in pairs if len(ko) < 30]    # çŸ­æ–‡
    medium = [(ko, ja) for ko, ja in pairs if 30 <= len(ko) < 60]  # ä¸­æ–‡
    long = [(ko, ja) for ko, ja in pairs if len(ko) >= 60]    # é•·æ–‡
    
    # æ¯”ç‡: çŸ­æ–‡30%, ä¸­æ–‡50%, é•·æ–‡20%
    n_short = int(n_samples * 0.3)
    n_medium = int(n_samples * 0.5)
    n_long = n_samples - n_short - n_medium
    
    samples = []
    samples.extend(random.sample(short, min(n_short, len(short))))
    samples.extend(random.sample(medium, min(n_medium, len(medium))))
    samples.extend(random.sample(long, min(n_long, len(long))))
    
    # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è£œå……
    if len(samples) < n_samples:
        remaining = [p for p in pairs if p not in samples]
        samples.extend(random.sample(remaining, min(n_samples - len(samples), len(remaining))))
    
    random.shuffle(samples)
    return samples[:n_samples]

def save_samples(samples: list[tuple[str, str]], output_dir: Path):
    """ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    ko_file = output_dir / "source_ko.txt"
    ja_file = output_dir / "reference_ja.txt"
    
    with open(ko_file, 'w', encoding='utf-8') as f:
        for ko, _ in samples:
            f.write(ko + '\n')
    
    with open(ja_file, 'w', encoding='utf-8') as f:
        for _, ja in samples:
            f.write(ja + '\n')
    
    # çµ±è¨ˆæƒ…å ±
    stats_file = output_dir / "stats.txt"
    ko_lengths = [len(ko) for ko, _ in samples]
    ja_lengths = [len(ja) for _, ja in samples]
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(samples)}\n")
        f.write(f"éŸ“å›½èªå¹³å‡é•·: {sum(ko_lengths)/len(ko_lengths):.1f}æ–‡å­—\n")
        f.write(f"éŸ“å›½èªæœ€å°/æœ€å¤§: {min(ko_lengths)}/{max(ko_lengths)}æ–‡å­—\n")
        f.write(f"æ—¥æœ¬èªå¹³å‡é•·: {sum(ja_lengths)/len(ja_lengths):.1f}æ–‡å­—\n")
        f.write(f"æ—¥æœ¬èªæœ€å°/æœ€å¤§: {min(ja_lengths)}/{max(ja_lengths)}æ–‡å­—\n")
    
    print(f"âœ… ä¿å­˜å®Œäº†: {output_dir}")
    print(f"   - {ko_file.name}: {len(samples)}è¡Œ")
    print(f"   - {ja_file.name}: {len(samples)}è¡Œ")
    print(f"   - {stats_file.name}")

def main():
    parser = argparse.ArgumentParser(description="è©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡º")
    parser.add_argument("--ko", type=Path, default=Path.home() / "grasp-models/data/cleaned/cleaned.ko")
    parser.add_argument("--ja", type=Path, default=Path.home() / "grasp-models/data/cleaned/cleaned.ja")
    parser.add_argument("--output", type=Path, default=Path.home() / "grasp-models/evaluation/samples")
    parser.add_argument("--n-samples", type=int, default=1000)
    parser.add_argument("--min-len", type=int, default=10)
    parser.add_argument("--max-len", type=int, default=100)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()
    
    print(f"ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {args.ko}")
    pairs = load_parallel_data(args.ko, args.ja)
    print(f"   å…¨ãƒšã‚¢æ•°: {len(pairs):,}")
    
    print(f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (é•·ã•: {args.min_len}-{args.max_len}æ–‡å­—)")
    filtered = filter_good_samples(pairs, args.min_len, args.max_len)
    print(f"   ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered):,}")
    
    print(f"ğŸ“Š å¤šæ§˜æ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (n={args.n_samples})")
    samples = select_diverse_samples(filtered, args.n_samples, args.seed)
    
    save_samples(samples, args.output)

if __name__ == "__main__":
    main()
