#!/usr/bin/env python3
"""
RunPodã§LLMç¿»è¨³è©•ä¾¡ã‚’ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

ä½¿ç”¨æ–¹æ³•:
  # vLLMã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•æ¸ˆã¿ã®çŠ¶æ…‹ã§
  python3 run_full_evaluation.py --base-url http://localhost:8000/v1
  
  # OpenRouterçµŒç”±
  OPENROUTER_API_KEY=xxx python3 run_full_evaluation.py --provider openrouter
"""

import os
import sys
import json
import time
import argparse
import subprocess
from pathlib import Path
from typing import Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import httpx

# ===== è¨­å®š =====
MODELS = [
    # (åå‰, ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã®ãƒ¢ãƒ‡ãƒ«å)
    ("qwen3-32b", "Qwen/Qwen3-32B"),
]

STRATEGIES = ["baseline", "natural", "few_shot"]

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥
PROMPT_STRATEGIES = {
    "baseline": {
        "system": "ã‚ãªãŸã¯éŸ“å›½èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³è€…ã§ã™ã€‚",
        "user": "æ¬¡ã®éŸ“å›½èªã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ç¿»è¨³ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n{text}"
    },
    "natural": {
        "system": """ã‚ãªãŸã¯éŸ“å›½èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³è€…ã§ã™ã€‚
è‡ªç„¶ã§æµæš¢ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
éŸ“å›½èªç‰¹æœ‰ã®è¡¨ç¾ã¯æ—¥æœ¬èªã¨ã—ã¦è‡ªç„¶ãªè¨€ã„å›ã—ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚
ã‚¢ãƒ‹ãƒ¡ã‚„ãƒ‰ãƒ©ãƒã®ã‚»ãƒªãƒ•ã®ã‚ˆã†ãªå£èªè¡¨ç¾ã‚’æ„è­˜ã—ã¦ãã ã•ã„ã€‚""",
        "user": "æ¬¡ã®éŸ“å›½èªã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ç¿»è¨³ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n{text}"
    },
    "few_shot": {
        "system": "ã‚ãªãŸã¯éŸ“å›½èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³è€…ã§ã™ã€‚ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚",
        "user": "æ¬¡ã®éŸ“å›½èªã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ç¿»è¨³ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n{text}",
        "examples": [
            ("ë­ í•˜ëŠ” ê±°ì•¼?", "ä½•ã—ã¦ã‚‹ã®ï¼Ÿ"),
            ("ì§„ì§œ ë¯¸ì¹˜ê² ë‹¤", "æœ¬å½“ã«ã©ã†ã‹ã—ã¦ã‚‹"),
            ("ê´œì°®ì•„, ê±±ì •í•˜ì§€ ë§ˆ", "å¤§ä¸ˆå¤«ã€å¿ƒé…ã—ãªã„ã§"),
        ]
    }
}


def load_lines(path: Path) -> list[str]:
    with open(path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f]


def build_messages(text: str, strategy: str) -> list[dict]:
    config = PROMPT_STRATEGIES[strategy]
    messages = [{"role": "system", "content": config["system"]}]
    
    if "examples" in config:
        for ko, ja in config["examples"]:
            messages.append({"role": "user", "content": f"éŸ“å›½èª: {ko}"})
            messages.append({"role": "assistant", "content": ja})
    
    messages.append({"role": "user", "content": config["user"].format(text=text)})
    return messages


def translate_text(client: httpx.Client, base_url: str, model: str, 
                   text: str, strategy: str) -> str:
    messages = build_messages(text, strategy)
    
    resp = client.post(
        f"{base_url}/chat/completions",
        json={
            "model": model,
            "messages": messages,
            "max_tokens": 256,
            "temperature": 0.3,
        },
        timeout=60.0
    )
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"].strip()


def translate_batch(texts: list[str], base_url: str, model: str, 
                   strategy: str, max_workers: int = 8) -> list[str]:
    results = [""] * len(texts)
    
    with httpx.Client() as client:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {}
            for i, text in enumerate(texts):
                future = executor.submit(
                    translate_text, client, base_url, model, text, strategy
                )
                futures[future] = i
            
            done = 0
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    results[idx] = future.result()
                except Exception as e:
                    print(f"  âš ï¸ Error at {idx}: {e}")
                    results[idx] = ""
                done += 1
                if done % 100 == 0:
                    print(f"  Progress: {done}/{len(texts)}")
    
    return results


def evaluate_translations(sources: list[str], references: list[str], 
                         hypotheses: list[str]) -> dict:
    metrics = {}
    
    # chrF++
    try:
        from sacrebleu import corpus_chrf
        result = corpus_chrf(hypotheses, [references], word_order=2)
        metrics['chrf_pp'] = round(result.score, 2)
    except Exception as e:
        print(f"  âš ï¸ chrF++ error: {e}")
    
    # BLEU
    try:
        from sacrebleu import corpus_bleu
        result = corpus_bleu(hypotheses, [references], tokenize='char')
        metrics['bleu'] = round(result.score, 2)
    except Exception as e:
        print(f"  âš ï¸ BLEU error: {e}")
    
    # COMET (GPU required)
    try:
        from comet import download_model, load_from_checkpoint
        
        print("  ğŸ“Š Loading COMET model...")
        model_path = download_model("Unbabel/wmt22-comet-da")
        model = load_from_checkpoint(model_path)
        
        data = [
            {"src": s, "mt": h, "ref": r}
            for s, h, r in zip(sources, hypotheses, references)
        ]
        
        output = model.predict(data, batch_size=16, gpus=1)
        metrics['comet'] = round(output.system_score, 4)
        
    except ImportError:
        print("  âš ï¸ COMET not installed")
    except Exception as e:
        print(f"  âš ï¸ COMET error: {e}")
    
    return metrics


def main():
    parser = argparse.ArgumentParser(description="LLMç¿»è¨³è©•ä¾¡ä¸€æ‹¬å®Ÿè¡Œ")
    parser.add_argument("--samples-dir", type=Path, default=Path("samples"))
    parser.add_argument("--output-dir", type=Path, default=Path("results"))
    parser.add_argument("--base-url", type=str, default="http://localhost:8000/v1")
    parser.add_argument("--provider", choices=["vllm", "openrouter"], default="vllm")
    parser.add_argument("--limit", type=int, help="å‡¦ç†è¡Œæ•°åˆ¶é™ (ãƒ‡ãƒãƒƒã‚°ç”¨)")
    parser.add_argument("--skip-translate", action="store_true", help="ç¿»è¨³ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè©•ä¾¡ã®ã¿ï¼‰")
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
    if args.provider == "openrouter":
        args.base_url = "https://openrouter.ai/api/v1"
        api_key = os.environ.get("OPENROUTER_API_KEY")
        if not api_key:
            print("âŒ OPENROUTER_API_KEY not set")
            sys.exit(1)
    
    # å…¥åŠ›èª­ã¿è¾¼ã¿
    source_file = args.samples_dir / "source_ko.txt"
    reference_file = args.samples_dir / "reference_ja.txt"
    
    sources = load_lines(source_file)
    references = load_lines(reference_file)
    
    if args.limit:
        sources = sources[:args.limit]
        references = references[:args.limit]
    
    print(f"ğŸ“¥ Loaded {len(sources)} samples")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    args.output_dir.mkdir(parents=True, exist_ok=True)
    translations_dir = Path("translations")
    translations_dir.mkdir(exist_ok=True)
    
    all_results = []
    
    for model_name, model_id in MODELS:
        for strategy in STRATEGIES:
            run_name = f"{model_name}-{strategy}"
            trans_file = translations_dir / f"{run_name}.txt"
            
            print(f"\n{'='*60}")
            print(f"ğŸ”„ {run_name}")
            print(f"{'='*60}")
            
            # ç¿»è¨³å®Ÿè¡Œ
            if not args.skip_translate or not trans_file.exists():
                print(f"  ğŸŒ Translating with {model_id}...")
                start = time.time()
                hypotheses = translate_batch(sources, args.base_url, model_id, strategy)
                elapsed = time.time() - start
                print(f"  â±ï¸ Translation took {elapsed:.1f}s ({len(sources)/elapsed:.1f} samples/s)")
                
                # ä¿å­˜
                with open(trans_file, 'w', encoding='utf-8') as f:
                    for h in hypotheses:
                        f.write(h + '\n')
                print(f"  ğŸ’¾ Saved to {trans_file}")
            else:
                print(f"  ğŸ“‚ Loading existing translations from {trans_file}")
                hypotheses = load_lines(trans_file)
            
            # è©•ä¾¡
            print(f"  ğŸ“Š Evaluating...")
            metrics = evaluate_translations(sources, references, hypotheses)
            metrics['model'] = model_name
            metrics['strategy'] = strategy
            metrics['n_samples'] = len(sources)
            
            all_results.append(metrics)
            
            print(f"  ğŸ“ˆ Results:")
            print(f"     chrF++: {metrics.get('chrf_pp', 'N/A')}")
            print(f"     BLEU:   {metrics.get('bleu', 'N/A')}")
            print(f"     COMET:  {metrics.get('comet', 'N/A')}")
    
    # çµæœã¾ã¨ã‚
    print(f"\n{'='*70}")
    print("ğŸ“Š FINAL COMPARISON")
    print(f"{'='*70}")
    print(f"{'Model':<20} {'Strategy':<12} {'chrF++':>10} {'BLEU':>10} {'COMET':>10}")
    print("-"*70)
    
    sorted_results = sorted(all_results, key=lambda x: x.get('chrf_pp', 0), reverse=True)
    for i, r in enumerate(sorted_results):
        rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
        print(f"{rank} {r['model']:<17} {r['strategy']:<12} "
              f"{r.get('chrf_pp', 'N/A'):>10} {r.get('bleu', 'N/A'):>10} "
              f"{r.get('comet', 'N/A'):>10}")
    
    # JSONä¿å­˜
    result_file = args.output_dir / "comparison.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'n_samples': len(sources),
            'results': all_results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Results saved to {result_file}")


if __name__ == "__main__":
    main()
