#!/bin/bash
# RunPodã§ã®LLMè©•ä¾¡ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

set -e

echo "ğŸš€ RunPod LLMè©•ä¾¡ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"

# 1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."
pip install -q vllm sacrebleu unbabel-comet httpx

# 2. è©•ä¾¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p /workspace/llm-eval/{samples,translations,results}

echo "âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†"
echo ""
echo "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:"
echo "1. ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
echo "   scp samples/source_ko.txt samples/reference_ja.txt root@IP:/workspace/llm-eval/samples/"
echo ""
echo "2. vLLMã‚µãƒ¼ãƒãƒ¼èµ·å‹• (åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«)"
echo "   vllm serve Qwen/Qwen3-32B --port 8000 --tensor-parallel-size 1"
echo ""
echo "3. ç¿»è¨³å®Ÿè¡Œ"
echo "   python3 translate_with_llm.py --provider vllm --base-url http://localhost:8000/v1 ..."
