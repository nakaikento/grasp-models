import torch
import re
import os
import argparse
from tqdm import tqdm
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# --- è¨­å®š ---
MODEL_NAME = "facebook/nllb-200-3.3b"
SOURCE_FILE = "data/raw/OpenSubtitles.ja-ko.ja"
OUTPUT_FILE = "data/teacher/train.ko"
SAMPLE_INTERVAL = 5000 

JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')
def contains_japanese(text):
    return bool(JP_PATTERN.search(text))

def clean_input(text):
    """ãƒ¢ãƒ‡ãƒ«ãŒæ··ä¹±ã—ã‚„ã™ã„è¨˜å·ã‚’ä¸€æ™‚çš„ã«é™¤å»"""
    t = text.replace('ãƒ»ãƒ»', '')
    t = t.replace('ãƒ»', '')
    t = re.sub(r'^[-ãƒ¼ï¼]\s*', '', t) # æ–‡é ­ã®ãƒã‚¤ãƒ•ãƒ³ç­‰ã‚’é™¤å»
    return t.strip()

def main():
    parser = argparse.ArgumentParser()
    # num_beams=3 ã«ã™ã‚‹ãŸã‚ã€å®‰å…¨ã‚’è¦‹ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°‘ã—èª¿æ•´ï¼ˆ32-40æ¨å¥¨ï¼‰
    parser.add_argument("--batch_size", type=int, default=40)
    args = parser.parse_args()

    print(f"ğŸš€ è¶…é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰(16-bit + Beam3 + Cleaning)ã§ãƒ­ãƒ¼ãƒ‰ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tgt_lang_id = tokenizer.convert_tokens_to_ids("kor_Hang")

    with open(SOURCE_FILE, 'r', encoding='utf-8') as f:
        ja_lines = [line.strip() for line in f]
    
    start_idx = 0
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            start_idx = sum(1 for _ in f)
        print(f"ğŸ”„ {start_idx}è¡Œç›®ã‹ã‚‰å†é–‹ã—ã¾ã™...")

    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        for i in tqdm(range(start_idx, len(ja_lines), args.batch_size), initial=start_idx//args.batch_size):
            batch_raw = ja_lines[i : i + args.batch_size]
            
            # ã€ä¿®æ­£ã€‘å…¥åŠ›æ–‡ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            batch_cleaned = [clean_input(line) if len(line) > 1 else line for line in batch_raw]
            
            # ç©ºè¡Œå¯¾ç­–ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã§ç©ºã«ãªã£ãŸå ´åˆç”¨ï¼‰
            batch_cleaned = [c if c else "ã€‚" for c in batch_cleaned]

            inputs = tokenizer(batch_cleaned, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    forced_bos_token_id=tgt_lang_id,
                    max_length=128,
                    num_beams=3,           # æ¢ç´¢ã®å¹…ã‚’ã•ã‚‰ã«å¼·åŒ–
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )
            
            results = tokenizer.batch_decode(outputs, skip_special_tokens=True)

            final_results = []
            for res in results:
                clean_res = res.replace("\n", " ").strip()
                # æ—¥æœ¬èªãŒæ®‹ã£ã¦ã„ã‚‹ã‹ã€æ¥µç«¯ã«çŸ­ã„ï¼ˆå¤±æ•—ï¼‰å ´åˆã¯å¼¾ã
                if contains_japanese(clean_res) or len(clean_res) < 1:
                    final_results.append("FAILED_TRANSLATION_CLEANED")
                else:
                    final_results.append(clean_res)

            if i % SAMPLE_INTERVAL < args.batch_size:
                print(f"\n--- [é€²æ—ãƒã‚§ãƒƒã‚¯: {i}è¡Œç›®] ---")
                print(f"åŸ: {batch_raw[0]}")
                print(f"æ´—: {batch_cleaned[0]}")
                print(f"éŸ“: {final_results[0]}")
                print("-" * 40)

            for res in final_results:
                f.write(res + "\n")
            f.flush()

            del inputs, outputs
            torch.cuda.empty_cache()

if __name__ == "__main__":
    main()