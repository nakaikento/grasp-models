import torch
import re
import os
import argparse
from tqdm import tqdm
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# --- è¨­å®š ---
MODEL_NAME = "facebook/nllb-200-3.3b"
SOURCE_FILE = "data/raw/OpenSubtitles.ja-ko.ja"
OUTPUT_FILE = "data/teacher/train.ko"
SAMPLE_INTERVAL = 10000 

JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')
def contains_japanese(text):
    return bool(JP_PATTERN.search(text))

def main():
    parser = argparse.ArgumentParser()
    # 16-bitã§ã¯ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ãŒå¢—ãˆã‚‹ãŸã‚ã€batch_sizeã¯32ã€œ64ã‚’æ¨å¥¨
    parser.add_argument("--batch_size", type=int, default=48)
    args = parser.parse_args()

    print(f"ğŸš€ é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰(16-bit bfloat16)ã§ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # ã€ä¿®æ­£ã®è‚ã€‘é‡å­åŒ–(BitsAndBytesConfig)ã‚’å¤–ã—ã€bfloat16ã‚’æŒ‡å®š
    model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,  # 4090ã«æœ€é©ãªç²¾åº¦
        device_map="auto"
    )
    
    tgt_lang_id = tokenizer.convert_tokens_to_ids("kor_Hang")

    with open(SOURCE_FILE, 'r', encoding='utf-8') as f:
        ja_lines = [line.strip() for line in f]
    
    start_idx = 0
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            start_idx = sum(1 for _ in f)
        print(f"ğŸ”„ {start_idx}è¡Œç›®ã‹ã‚‰å†é–‹ã—ã¾ã™...")

    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        for i in tqdm(range(start_idx, len(ja_lines), args.batch_size), initial=start_idx//args.batch_size):
            batch = ja_lines[i : i + args.batch_size]
            
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")

            with torch.no_grad():
                # ã€ä¿®æ­£ã®è‚ã€‘Beam Search (num_beams=2) ã‚’æœ‰åŠ¹åŒ–
                outputs = model.generate(
                    **inputs,
                    forced_bos_token_id=tgt_lang_id,
                    max_length=128,
                    num_beams=2,           # å€™è£œã‚’2ã¤æ¢ç´¢ã—ã¦è³ªã®é«˜ã„æ–¹ã‚’é¸æŠ
                    no_repeat_ngram_size=3, # ç¹°ã‚Šè¿”ã—ãƒã‚°é˜²æ­¢
                    early_stopping=True
                )
            
            results = tokenizer.batch_decode(outputs, skip_special_tokens=True)

            # æ›¸ãå‡ºã—ç”¨ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨æ—¥æœ¬èªãƒã‚§ãƒƒã‚¯
            final_results = []
            for res in results:
                clean_res = res.replace("\n", " ").strip()
                if contains_japanese(clean_res) or not clean_res:
                    final_results.append("FAILED_TRANSLATION_CLEANED")
                else:
                    final_results.append(clean_res)

            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            if i % SAMPLE_INTERVAL < args.batch_size:
                print(f"\n--- [é€²æ—ãƒã‚§ãƒƒã‚¯: {i}è¡Œç›®] ---")
                print(f"æ—¥: {batch[0]}")
                print(f"éŸ“: {final_results[0]}")
                print("-" * 40)

            for res in final_results:
                f.write(res + "\n")
            f.flush()

            # ãƒ¡ãƒ¢ãƒªã®æ˜ç¤ºçš„è§£æ”¾ï¼ˆOOMå¯¾ç­–ï¼‰
            del inputs, outputs
            torch.cuda.empty_cache()

if __name__ == "__main__":
    main()