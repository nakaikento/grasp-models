#!/usr/bin/env python3
"""
æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ3.3Bãƒ¢ãƒ‡ãƒ« + ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯æ­è¼‰ç‰ˆï¼‰

NLLB-200-3.3Bã‚’ä½¿ç”¨ã—ã¦é«˜å“è³ªãªç¿»è¨³ã‚’ç”Ÿæˆã€‚
4-bité‡å­åŒ–ã«ã‚ˆã‚ŠL4/T4 GPUã«å¯¾å¿œã—ã€æ—¥æœ¬èªæ··å…¥æ™‚ã«è¨­å®šã‚’å¤‰ãˆã¦å†è©¦è¡Œã—ã¾ã™ã€‚
"""

import torch
import re
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
from dataclasses import dataclass
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig

# æ—¥æœ¬èªæ¤œçŸ¥ç”¨æ­£è¦è¡¨ç¾ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ï¼‰
JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')

def contains_japanese(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    return bool(JP_PATTERN.search(text))

def load_model_optimized(model_name: str):
    """3.3Bãƒ¢ãƒ‡ãƒ«ã‚’4-bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰"""
    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name} (4-bit quantization)")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    
    return model, tokenizer

def count_existing_lines(file_path: Path) -> int:
    if not file_path.exists():
        return 0
    with open(file_path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--resume", action="store_true", help="é€”ä¸­ã‹ã‚‰å†é–‹")
    parser.add_argument("--model", type=str, default="facebook/nllb-200-3.3B")
    parser.add_argument("--batch_size", type=int, default=32) # L4å‘ã‘ã«32ã‚’è¨­å®š
    parser.add_argument("--num_beams", type=int, default=1)  # é€Ÿåº¦é‡è¦–
    parser.add_argument("--input", type=str, default="data/splits/train.ja")
    parser.add_argument("--output", type=str, default="data/teacher/train.ko")
    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    if not input_path.exists():
        print(f"âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
        return
    
    with open(input_path, "r", encoding="utf-8") as f:
        ja_texts = [line.strip() for line in f if line.strip()]

    # å†é–‹å‡¦ç†
    start_idx = 0
    if args.resume:
        start_idx = count_existing_lines(output_path)
        if start_idx > 0:
            print(f"ğŸ”„ å†é–‹ãƒ¢ãƒ¼ãƒ‰: {start_idx:,}è¡Œç›®ã‹ã‚‰é–‹å§‹ã—ã¾ã™")
            ja_texts = ja_texts[start_idx:]
    else:
        if output_path.exists():
            print(f"âš ï¸ è­¦å‘Š: {output_path} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚ä¸Šæ›¸ãã—ã¾ã™ã€‚")
            output_path.unlink()

    if not ja_texts:
        print("âœ… å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, tokenizer = load_model_optimized(args.model)
    tgt_lang = "kor_Hang"
    tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang)

    print(f"\nç¿»è¨³é–‹å§‹ (Target: {tgt_lang}, Batch Size: {args.batch_size})...")
    
    

    with open(output_path, "a", encoding="utf-8") as f:
        for i in tqdm(range(0, len(ja_texts), args.batch_size)):
            batch = ja_texts[i : i + args.batch_size]
            
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")
            
            # 1å›ç›®ã®ç”Ÿæˆ (é€šå¸¸ã®æ¨è«–)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    forced_bos_token_id=tgt_lang_id,
                    max_length=128,
                    num_beams=args.num_beams,
                    do_sample=False
                )
            
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            
            final_results = []
            for idx, (original_ja, translated_ko) in enumerate(zip(batch, decoded)):
                # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if contains_japanese(translated_ko):
                    # --- ãƒªãƒˆãƒ©ã‚¤è©¦è¡Œ (Samplingãƒ¢ãƒ¼ãƒ‰ã§1å›ã ã‘æŒ‘æˆ¦) ---
                    retry_input = tokenizer([original_ja], return_tensors="pt").to("cuda")
                    with torch.no_grad():
                        retry_output = model.generate(
                            **retry_input,
                            forced_bos_token_id=tgt_lang_id,
                            max_length=128,
                            do_sample=True,      # ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’å°å…¥
                            temperature=0.7,     # æŸ”è»Ÿãªç”Ÿæˆ
                            top_p=0.9,
                            num_beams=1
                        )
                    retry_text = tokenizer.decode(retry_output[0], skip_special_tokens=True)
                    
                    if contains_japanese(retry_text):
                        final_results.append("FAILED_TRANSLATION_CLEANED")
                    else:
                        final_results.append(retry_text)
                else:
                    final_results.append(translated_ko)

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
            for res in final_results:
                f.write(res + "\n")

    print(f"\nâœ¨ å®Œäº†! å‡ºåŠ›å…ˆ: {output_path}")

if __name__ == "__main__":
    main()