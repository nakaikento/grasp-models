#!/usr/bin/env python3
import torch
import re
import os
import argparse
from pathlib import Path
from tqdm import tqdm
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig

# --- å®šæ•°è¨­å®š ---
MODEL_NAME = "facebook/nllb-200-3.3b"
SOURCE_FILE = "data/raw/OpenSubtitles.ja-ko.ja"
OUTPUT_FILE = "data/teacher/train.ko"
SAMPLE_INTERVAL = 10000  # 1ä¸‡è¡Œã”ã¨ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º

# æ—¥æœ¬èªæ¤œçŸ¥ç”¨
JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')
def contains_japanese(text):
    return bool(JP_PATTERN.search(text))

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch_size", type=int, default=256)
    args = parser.parse_args()

    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # 4-bité‡å­åŒ–è¨­å®šï¼ˆVRAMç¯€ç´„ã¨é«˜é€ŸåŒ–ï¼‰
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    tgt_lang_id = tokenizer.lang_code_to_id["kor_Hang"]

    # åŸæ–‡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    if not os.path.exists(SOURCE_FILE):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {SOURCE_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    with open(SOURCE_FILE, 'r', encoding='utf-8') as f:
        ja_lines = [line.strip() for line in f]
    
    total_lines = len(ja_lines)
    print(f"ğŸ“– ç·è¡Œæ•°: {total_lines}")

    # å†é–‹ãƒã‚¤ãƒ³ãƒˆã®ç¢ºèªï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼‰
    start_idx = 0
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            start_idx = sum(1 for _ in f)
        print(f"ğŸ”„ {start_idx}è¡Œç›®ã‹ã‚‰å†é–‹ã—ã¾ã™ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨åŒæœŸï¼‰")
    else:
        # æ–°è¦ä½œæˆæ™‚ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯ä½œæˆ
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # ç¿»è¨³ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        # tqdmã§é€²æ—ã‚’è¡¨ç¤º
        for i in tqdm(range(start_idx, total_lines, args.batch_size), initial=start_idx//args.batch_size):
            batch = ja_lines[i : i + args.batch_size]
            
            # 1. ç¿»è¨³å®Ÿè¡Œ
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")
            with torch.no_grad():
                outputs = model.generate(
                    **inputs, 
                    forced_bos_token_id=tgt_lang_id, 
                    max_length=128
                )
            results = tokenizer.batch_decode(outputs, skip_special_tokens=True)

            # 2. æ—¥æœ¬èªãŒæ··ã˜ã£ãŸå ´åˆã®ç°¡æ˜“ãƒªãƒˆãƒ©ã‚¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            # â€»ä»Šå›ã¯ã€Œè¡Œã®åŒæœŸã€ã‚’æœ€å„ªå…ˆã™ã‚‹ãŸã‚ã€å¤±æ•—ã—ã¦ã‚‚å¿…ãš1è¡Œæ›¸ãå‡ºã—ã¾ã™
            final_results = []
            for idx, res in enumerate(results):
                clean_res = res.replace("\n", " ").strip()
                if contains_japanese(clean_res) or not clean_res:
                    final_results.append("FAILED_TRANSLATION_CLEANED")
                else:
                    final_results.append(clean_res)

            # 3. 1ä¸‡è¡Œã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼ˆå®‰å¿ƒæ©Ÿèƒ½ï¼‰
            if i % SAMPLE_INTERVAL < args.batch_size:
                print(f"\n\n--- [é€²æ—ãƒã‚§ãƒƒã‚¯: {i}è¡Œç›®] ---")
                print(f"æ—¥: {batch[0]}")
                print(f"éŸ“: {final_results[0]}")
                print("-" * 40)

            # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã¸æ›¸ãå‡ºã—
            for res in final_results:
                f.write(res + "\n")
            
            # ãƒãƒƒãƒ•ã‚¡ã‚’å¼·åˆ¶ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼ˆã‚¹ãƒªãƒ¼ãƒ—å¯¾ç­–ï¼‰
            f.flush()

    print(f"âœ¨ å®Œäº†ã—ã¾ã—ãŸï¼å‡ºåŠ›å…ˆ: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()