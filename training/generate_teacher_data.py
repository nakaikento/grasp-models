#!/usr/bin/env python3
"""
æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ3.3Bãƒ¢ãƒ‡ãƒ« + ãƒãƒƒãƒãƒ»ãƒªãƒˆãƒ©ã‚¤æœ€é©åŒ–ç‰ˆï¼‰

NLLB-200-3.3Bã‚’ä½¿ç”¨ã€‚
ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹ã“ã¨ã§ã€æ—¥æœ¬èªæ··å…¥æ™‚ã®é€Ÿåº¦ä½ä¸‹ã‚’åŠ‡çš„ã«æ”¹å–„ã—ã¾ã—ãŸã€‚
"""

import torch
import re
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig

# æ—¥æœ¬èªæ¤œçŸ¥ç”¨æ­£è¦è¡¨ç¾ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ï¼‰
JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')

def contains_japanese(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    return bool(JP_PATTERN.search(text))

def load_model_optimized(model_name: str):
    """3.3Bãƒ¢ãƒ‡ãƒ«ã‚’4-bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰"""
    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name} (4-bit quantization)")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    
    return model, tokenizer

def count_existing_lines(file_path: Path) -> int:
    if not file_path.exists():
        return 0
    with open(file_path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--resume", action="store_true", help="é€”ä¸­ã‹ã‚‰å†é–‹")
    parser.add_argument("--model", type=str, default="facebook/nllb-200-3.3B")
    parser.add_argument("--batch_size", type=int, default=64) # L4å‘ã‘ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’64ã«
    parser.add_argument("--num_beams", type=int, default=1)  # é€Ÿåº¦é‡è¦–
    parser.add_argument("--input", type=str, default="data/splits/train.ja")
    parser.add_argument("--output", type=str, default="data/teacher/train.ko")
    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    if not input_path.exists():
        print(f"âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
        return
    
    with open(input_path, "r", encoding="utf-8") as f:
        ja_texts = [line.strip() for line in f if line.strip()]

    # å†é–‹å‡¦ç†
    start_idx = 0
    if args.resume:
        start_idx = count_existing_lines(output_path)
        if start_idx > 0:
            print(f"ğŸ”„ å†é–‹ãƒ¢ãƒ¼ãƒ‰: {start_idx:,}è¡Œç›®ã‹ã‚‰é–‹å§‹ã—ã¾ã™")
            ja_texts = ja_texts[start_idx:]
    else:
        if output_path.exists():
            print(f"âš ï¸ è­¦å‘Š: {output_path} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚ä¸Šæ›¸ãã—ã¾ã™ã€‚")
            output_path.unlink()

    if not ja_texts:
        print("âœ… å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, tokenizer = load_model_optimized(args.model)
    tgt_lang = "kor_Hang"
    tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang)

    print(f"\nç¿»è¨³é–‹å§‹ (Target: {tgt_lang}, Batch Size: {args.batch_size})...")
    
    with open(output_path, "a", encoding="utf-8") as f:
        for i in tqdm(range(0, len(ja_texts), args.batch_size)):
            batch = ja_texts[i : i + args.batch_size]
            
            # 1. ãƒ¡ã‚¤ãƒ³ç¿»è¨³ (ä¸€æ‹¬ãƒãƒƒãƒ)
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    forced_bos_token_id=tgt_lang_id,
                    max_length=128,
                    num_beams=args.num_beams,
                    do_sample=False
                )
            results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            
            # 2. æ—¥æœ¬èªãŒæ··ã˜ã£ãŸè¡Œã‚’ç‰¹å®š
            retry_indices = [idx for idx, text in enumerate(results) if contains_japanese(text)]
            
            # 3. å¤±æ•—ã—ãŸè¡Œã ã‘ã‚’ã¾ã¨ã‚ã¦ãƒãƒƒãƒãƒªãƒˆãƒ©ã‚¤
            if retry_indices:
                retry_inputs_texts = [batch[idx] for idx in retry_indices]
                retry_inputs = tokenizer(retry_inputs_texts, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")
                
                with torch.no_grad():
                    retry_outputs = model.generate(
                        **retry_inputs,
                        forced_bos_token_id=tgt_lang_id,
                        max_length=128,
                        do_sample=True,      # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å¤šæ§˜æ€§ã‚’æŒãŸã›ã‚‹
                        temperature=0.7,
                        top_p=0.9,
                        num_beams=1
                    )
                retry_results = tokenizer.batch_decode(retry_outputs, skip_special_tokens=True)
                
                # çµæœã‚’å·®ã—æ›¿ãˆã€ãã‚Œã§ã‚‚æ—¥æœ¬èªãªã‚‰ FAILED ã«ã™ã‚‹
                for idx, retry_text in zip(retry_indices, retry_results):
                    if contains_japanese(retry_text):
                        results[idx] = "FAILED_TRANSLATION_CLEANED"
                    else:
                        results[idx] = retry_text

            # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸€æ–‰æ›¸ãå‡ºã—
            for res in results:
                f.write(res + "\n")

    print(f"\nâœ¨ å®Œäº†! å‡ºåŠ›å…ˆ: {output_path}")

if __name__ == "__main__":
    main()