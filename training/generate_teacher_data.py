#!/usr/bin/env python3
"""
æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ3.3Bãƒ¢ãƒ‡ãƒ« & æ—¥æœ¬èªãƒ•ã‚£ãƒ«ã‚¿æ­è¼‰ç‰ˆï¼‰

NLLB-200-3.3Bã‚’ä½¿ç”¨ã—ã¦é«˜å“è³ªãªç¿»è¨³ã‚’ç”Ÿæˆã€‚
4-bité‡å­åŒ–ã«ã‚ˆã‚ŠColab/Consumer GPUã§ã®å‹•ä½œã«å¯¾å¿œã—ã€
æ—¥æœ¬èªãŒæ··å…¥ã—ãŸè¡Œã‚’è‡ªå‹•çš„ã«é™¤å¤–ãƒ»å†è©¦è¡Œã—ã¾ã™ã€‚
"""

import torch
import re
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
from dataclasses import dataclass
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig

# è¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ‘ã‚¹ãŒé€šã‚‹ã‚ˆã†ã«èª¿æ•´ï¼‰
sys.path.append(str(Path(__file__).parent.parent))
# from training.config import DistillationConfig # å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤

@dataclass
class GenerationArgs:
    input_file: Path = Path("data/splits/train.ja")
    output_file: Path = Path("data/teacher/train.ko")
    model_name: str = "facebook/nllb-200-3.3B"
    batch_size: int = 4  # 3.3Bç”¨ã«å°ã•ãèª¿æ•´
    max_length: int = 128
    num_beams: int = 2   # é€Ÿåº¦ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    save_every: int = 10 # 10ãƒãƒƒãƒã”ã¨ã«ä¿å­˜

# æ—¥æœ¬èªæ¤œçŸ¥ç”¨æ­£è¦è¡¨ç¾
JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')

def contains_japanese(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã«æ—¥æœ¬èªï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š"""
    return bool(JP_PATTERN.search(text))

def load_model_optimized(model_name: str, device: str):
    """3.3Bãƒ¢ãƒ‡ãƒ«ã‚’4-bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰"""
    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name} (4-bit quantization)")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Colabã®T4 GPUã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«4bité‡å­åŒ–ã‚’è¨­å®š
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    
    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto", # è‡ªå‹•ã§GPUã«å‰²ã‚Šå½“ã¦
        torch_dtype=torch.float16,
    )
    
    return model, tokenizer

def count_existing_lines(file_path: Path) -> int:
    if not file_path.exists():
        return 0
    with open(file_path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--resume", action="store_true", help="é€”ä¸­ã‹ã‚‰å†é–‹")
    parser.add_argument("--model", type=str, default="facebook/nllb-200-3.3B")
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--num_beams", type=int, default=2)
    parser.add_argument("--input", type=str, default="data/splits/train.ja")
    parser.add_argument("--output", type=str, default="data/teacher/train.ko")
    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    if not input_path.exists():
        print(f"âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
        return
    
    with open(input_path, "r", encoding="utf-8") as f:
        ja_texts = [line.strip() for line in f if line.strip()]

    # å†é–‹å‡¦ç†
    start_idx = 0
    if args.resume:
        start_idx = count_existing_lines(output_path)
        if start_idx > 0:
            print(f"ğŸ”„ å†é–‹ãƒ¢ãƒ¼ãƒ‰: {start_idx:,}è¡Œç›®ã‹ã‚‰é–‹å§‹ã—ã¾ã™")
            ja_texts = ja_texts[start_idx:]
    else:
        if output_path.exists():
            print(f"âš ï¸ è­¦å‘Š: {output_path} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚ä¸Šæ›¸ãã—ã¾ã™ã€‚")
            output_path.unlink()

    if not ja_texts:
        print("âœ… å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, tokenizer = load_model_optimized(args.model, "cuda")
    tgt_lang = "kor_Hang"

    # ç”Ÿæˆé–‹å§‹
    print(f"\nç¿»è¨³é–‹å§‹ (Target: {tgt_lang})...")
    
    with open(output_path, "a", encoding="utf-8") as f:
        for i in tqdm(range(0, len(ja_texts), args.batch_size)):
            batch = ja_texts[i : i + args.batch_size]
            
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=128).to("cuda")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),
                    max_length=128,
                    num_beams=args.num_beams
                )
            
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            
            for original_ja, translated_ko in zip(batch, decoded):
                # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if contains_japanese(translated_ko):
                    # æ—¥æœ¬èªãŒæ··ã˜ã£ãŸå ´åˆã¯ã€ç©ºè¡Œã«ã™ã‚‹ã‹ã‚¨ãƒ©ãƒ¼ç”¨ã®å°ã‚’ä»˜ã‘ã¦
                    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®å“è³ªã‚’å®ˆã‚‹
                    f.write("FAILED_TRANSLATION_CLEANED\n")
                else:
                    f.write(translated_ko + "\n")

    print(f"\nâœ¨ å®Œäº†! å‡ºåŠ›å…ˆ: {output_path}")

if __name__ == "__main__":
    main()