#!/usr/bin/env python3
"""
æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé€”ä¸­ä¿å­˜å¯¾å¿œç‰ˆï¼‰

M2M100 or NLLB-200ã‚’ä½¿ç”¨ã—ã¦ã€æ—¥æœ¬èªã‹ã‚‰é«˜å“è³ªãªéŸ“å›½èªç¿»è¨³ã‚’ç”Ÿæˆ

Usage:
    python training/generate_teacher_data.py
    python training/generate_teacher_data.py --resume  # é€”ä¸­ã‹ã‚‰å†é–‹

Input:  data/splits/train.ja
Output: data/teacher/train.ko (æ•™å¸«ç¿»è¨³)
"""

import torch
from pathlib import Path
from tqdm import tqdm
from dataclasses import dataclass
import argparse

# è¨­å®šã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append(str(Path(__file__).parent.parent))
from training.config import DistillationConfig


@dataclass
class GenerationArgs:
    input_file: Path = Path("data/splits/train.ja")
    output_file: Path = Path("data/teacher/train.ko")
    model_name: str = "facebook/nllb-200-1.3B"
    batch_size: int = 16
    max_length: int = 128
    num_beams: int = 5
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    resume_from: int = 0  # é€”ä¸­ã‹ã‚‰å†é–‹ã™ã‚‹å ´åˆ


def load_model(model_name: str, device: str):
    """æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name}")
    
    # accelerateã‚’ä½¿ã‚ãšã«ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ­ãƒ¼ãƒ‰
    import os
    os.environ["ACCELERATE_USE_SAFETENSORS"] = "true"
    
    if "nllb" in model_name.lower():
        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆdevice_mapãªã—ï¼‰
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
        )
        if device == "cuda":
            model = model.half()  # GPUæ™‚ã¯fp16
        model = model.to(device)
        
        src_lang = "jpn_Jpan"
        tgt_lang = "kor_Hang"
        
    elif "m2m100" in model_name.lower():
        from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
        tokenizer = M2M100Tokenizer.from_pretrained(model_name)
        
        model = M2M100ForConditionalGeneration.from_pretrained(
            model_name,
            use_safetensors=True,
            low_cpu_mem_usage=False,
        )
        if device == "cuda":
            model = model.half()
        model = model.to(device)
        
        tokenizer.src_lang = "ja"
        src_lang = "ja"
        tgt_lang = "ko"
    else:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«: {model_name}")
    
    model.eval()
    
    return model, tokenizer, src_lang, tgt_lang


def count_existing_lines(output_path: Path) -> int:
    """æ—¢å­˜ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    if not output_path.exists():
        return 0
    with open(output_path, 'r', encoding='utf-8') as f:
        return sum(1 for line in f if line.strip())


def generate_translations(
    model,
    tokenizer,
    texts: list,
    tgt_lang: str,
    batch_size: int,
    max_length: int,
    num_beams: int,
    device: str,
    model_name: str,
    output_path: Path,
    save_every: int = 100,  # 100ãƒãƒƒãƒã”ã¨ã«ä¿å­˜
    start_idx: int = 0,
):
    """ãƒãƒƒãƒå‡¦ç†ã§ç¿»è¨³ã‚’ç”Ÿæˆï¼ˆé€”ä¸­ä¿å­˜å¯¾å¿œï¼‰"""
    translations = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§é–‹ãæº–å‚™
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    for batch_idx, i in enumerate(tqdm(range(0, len(texts), batch_size), desc="ç¿»è¨³ç”Ÿæˆä¸­")):
        batch = texts[i:i + batch_size]
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            if "nllb" in model_name.lower():
                generated = model.generate(
                    **inputs,
                    forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),
                    max_length=max_length,
                    num_beams=num_beams,
                    early_stopping=True,
                )
            else:  # M2M100
                generated = model.generate(
                    **inputs,
                    forced_bos_token_id=tokenizer.get_lang_id(tgt_lang),
                    max_length=max_length,
                    num_beams=num_beams,
                    early_stopping=True,
                )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)
        translations.extend(decoded)
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del inputs, generated
        if device == "cuda":
            torch.cuda.empty_cache()
        
        # å®šæœŸä¿å­˜
        if (batch_idx + 1) % save_every == 0:
            # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜
            with open(output_path, 'a', encoding='utf-8') as f:
                f.write('\n'.join(translations) + '\n')
            
            total_saved = start_idx + (batch_idx + 1) * batch_size
            print(f"\nğŸ’¾ é€”ä¸­ä¿å­˜: {total_saved:,}è¡Œ ({(batch_idx + 1) / total_batches * 100:.1f}%)")
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            translations = []
    
    # æ®‹ã‚Šã‚’ä¿å­˜
    if translations:
        with open(output_path, 'a', encoding='utf-8') as f:
            f.write('\n'.join(translations) + '\n')
    
    return len(texts)


def main():
    parser = argparse.ArgumentParser(description="æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
    parser.add_argument("--input", type=str, default="data/splits/train.ja")
    parser.add_argument("--output", type=str, default="data/teacher/train.ko")
    parser.add_argument("--model", type=str, default="facebook/nllb-200-1.3B")
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--max-length", type=int, default=128)
    parser.add_argument("--num-beams", type=int, default=5)
    parser.add_argument("--resume", action="store_true", help="é€”ä¸­ã‹ã‚‰å†é–‹")
    parser.add_argument("--save-every", type=int, default=100, help="ä½•ãƒãƒƒãƒã”ã¨ã«ä¿å­˜ã™ã‚‹ã‹")
    args = parser.parse_args()
    
    print("=" * 50)
    print("æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
    print("=" * 50)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, tokenizer, src_lang, tgt_lang = load_model(args.model, device)
    print(f"è¨€èªãƒšã‚¢: {src_lang} â†’ {tgt_lang}")
    
    # å…¥åŠ›èª­ã¿è¾¼ã¿
    input_path = Path(args.input)
    output_path = Path(args.output)
    print(f"\nå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_path}")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        ja_texts = [line.strip() for line in f]
    
    total_lines = len(ja_texts)
    print(f"å…¥åŠ›è¡Œæ•°: {total_lines:,}")
    
    # é€”ä¸­ã‹ã‚‰å†é–‹
    start_idx = 0
    if args.resume:
        start_idx = count_existing_lines(output_path)
        if start_idx > 0:
            print(f"\nğŸ”„ å†é–‹ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜ {start_idx:,}è¡Œ ã‚’æ¤œå‡º")
            print(f"   è¡Œ {start_idx + 1} ã‹ã‚‰å†é–‹ã—ã¾ã™")
            ja_texts = ja_texts[start_idx:]
        else:
            print("\næ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ã€‚æœ€åˆã‹ã‚‰é–‹å§‹ã—ã¾ã™ã€‚")
    else:
        # æ–°è¦é–‹å§‹ã®å ´åˆã¯æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢
        if output_path.exists():
            output_path.unlink()
    
    if not ja_texts:
        print("\nâœ… ã™ã¹ã¦å®Œäº†æ¸ˆã¿ã§ã™ï¼")
        return
    
    # ç¿»è¨³ç”Ÿæˆ
    print(f"\nç¿»è¨³ç”Ÿæˆé–‹å§‹...")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}")
    print(f"  ãƒ“ãƒ¼ãƒ æ•°: {args.num_beams}")
    print(f"  ä¿å­˜é–“éš”: {args.save_every}ãƒãƒƒãƒã”ã¨")
    print(f"  æ®‹ã‚Š: {len(ja_texts):,}è¡Œ")
    
    num_generated = generate_translations(
        model=model,
        tokenizer=tokenizer,
        texts=ja_texts,
        tgt_lang=tgt_lang,
        batch_size=args.batch_size,
        max_length=args.max_length,
        num_beams=args.num_beams,
        device=device,
        model_name=args.model,
        output_path=output_path,
        save_every=args.save_every,
        start_idx=start_idx,
    )
    
    # æœ€çµ‚ç¢ºèª
    final_count = count_existing_lines(output_path)
    print(f"\nâœ… ä¿å­˜å®Œäº†: {output_path}")
    print(f"   ç·è¡Œæ•°: {final_count:,} / {total_lines:,}")
    
    if final_count >= total_lines:
        print("\nğŸ‰ ã™ã¹ã¦ã®ç¿»è¨³ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    else:
        print(f"\nâš ï¸  æ®‹ã‚Š {total_lines - final_count:,}è¡Œ")
        print("   --resume ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å†é–‹ã§ãã¾ã™")


if __name__ == "__main__":
    main()