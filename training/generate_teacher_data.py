import torch
import re
import os
import argparse
from tqdm import tqdm
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# NLLBè¨€èªã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
LANG_CODES = {
    "ja": "jpn_Jpan",
    "ko": "kor_Hang"
}

# è¨€èªæ¤œå‡ºç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')
KO_PATTERN = re.compile(r'[ê°€-í£]')

def contains_language(text, lang_code):
    """æŒ‡å®šã•ã‚ŒãŸè¨€èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    if lang_code == "ja":
        return bool(JP_PATTERN.search(text))
    elif lang_code == "ko":
        return bool(KO_PATTERN.search(text))
    return False

def clean_input(text):
    """ãƒ¢ãƒ‡ãƒ«ãŒæ··ä¹±ã—ã‚„ã™ã„è¨˜å·ã‚’ä¸€æ™‚çš„ã«é™¤å»"""
    t = text.replace('ãƒ»ãƒ»', '')
    t = t.replace('ãƒ»', '')
    t = re.sub(r'^[-ãƒ¼ï¼]\s*', '', t)  # æ–‡é ­ã®ãƒã‚¤ãƒ•ãƒ³ç­‰ã‚’é™¤å»
    return t.strip()

def main():
    parser = argparse.ArgumentParser(description="NLLB-200ã‚’ä½¿ç”¨ã—ãŸæ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæ±ç”¨ç‰ˆï¼‰")
    
    # è¨€èªè¨­å®š
    parser.add_argument("--src_lang", type=str, required=True, 
                        choices=["ja", "ko"],
                        help="ã‚½ãƒ¼ã‚¹è¨€èª (ja: æ—¥æœ¬èª, ko: éŸ“å›½èª)")
    parser.add_argument("--tgt_lang", type=str, required=True,
                        choices=["ja", "ko"],
                        help="ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª (ja: æ—¥æœ¬èª, ko: éŸ“å›½èª)")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    parser.add_argument("--src_file", type=str, required=True,
                        help="å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: data/raw/OpenSubtitles.ja-ko.ja)")
    parser.add_argument("--output_file", type=str, required=True,
                        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: data/teacher/train.ko)")
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument("--model_name", type=str, 
                        default="facebook/nllb-200-3.3b",
                        help="NLLBãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--batch_size", type=int, default=40,
                        help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument("--num_beams", type=int, default=3,
                        help="ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®å¹…")
    parser.add_argument("--max_length", type=int, default=128,
                        help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·")
    parser.add_argument("--sample_interval", type=int, default=5000,
                        help="é€²æ—è¡¨ç¤ºã®é–“éš”")
    
    args = parser.parse_args()
    
    # è¨€èªã‚³ãƒ¼ãƒ‰å–å¾—
    src_lang_code = LANG_CODES[args.src_lang]
    tgt_lang_code = LANG_CODES[args.tgt_lang]
    
    print(f"ğŸš€ è¨­å®š:")
    print(f"  ã‚½ãƒ¼ã‚¹è¨€èª: {args.src_lang} ({src_lang_code})")
    print(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª: {args.tgt_lang} ({tgt_lang_code})")
    print(f"  å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.src_file}")
    print(f"  å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.output_file}")
    print(f"  ãƒ¢ãƒ‡ãƒ«: {args.model_name}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}, ãƒ“ãƒ¼ãƒ : {args.num_beams}")
    print()
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸš€ è¶…é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰(16-bit + Beam{args.num_beams} + Cleaning)ã§ãƒ­ãƒ¼ãƒ‰ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang_code)
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“– å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {args.src_file}")
    if not os.path.exists(args.src_file):
        raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.src_file}")
    
    with open(args.src_file, 'r', encoding='utf-8') as f:
        src_lines = [line.strip() for line in f]
    
    print(f"âœ… {len(src_lines):,}è¡Œèª­ã¿è¾¼ã¿å®Œäº†")
    
    # å†é–‹å‡¦ç†
    start_idx = 0
    if os.path.exists(args.output_file):
        with open(args.output_file, 'r', encoding='utf-8') as f:
            start_idx = sum(1 for _ in f)
        print(f"ğŸ”„ {start_idx:,}è¡Œç›®ã‹ã‚‰å†é–‹ã—ã¾ã™...")
    else:
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    
    # ç¿»è¨³å®Ÿè¡Œ
    print(f"ğŸ”¥ ç¿»è¨³é–‹å§‹...")
    with open(args.output_file, 'a', encoding='utf-8') as f:
        for i in tqdm(range(start_idx, len(src_lines), args.batch_size), 
                      initial=start_idx//args.batch_size,
                      desc="ç¿»è¨³ä¸­"):
            batch_raw = src_lines[i : i + args.batch_size]
            
            # å…¥åŠ›æ–‡ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            batch_cleaned = [clean_input(line) if len(line) > 1 else line 
                             for line in batch_raw]
            
            # ç©ºè¡Œå¯¾ç­–
            batch_cleaned = [c if c else "ã€‚" for c in batch_cleaned]
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã ã‘ã§ãªãã€ã‚½ãƒ¼ã‚¹è¨€èª(src_lang)ã‚‚æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¾ã™
            tokenizer.src_lang = src_lang_code
            inputs = tokenizer(batch_cleaned, return_tensors="pt", 
                               padding=True, truncation=True, 
                               max_length=args.max_length).to("cuda")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    forced_bos_token_id=tgt_lang_id,
                    max_length=args.max_length,
                    num_beams=args.num_beams,
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )
            
            results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            
            final_results = []
            for res in results:
                clean_res = res.replace("\n", " ").strip()
                
                # ã‚½ãƒ¼ã‚¹è¨€èªãŒæ®‹ã£ã¦ã„ã‚‹ã€ã¾ãŸã¯æ¥µç«¯ã«çŸ­ã„å ´åˆã¯å¤±æ•—ã¨ãƒãƒ¼ã‚¯
                if contains_language(clean_res, args.src_lang) or len(clean_res) < 1:
                    final_results.append("FAILED_TRANSLATION_CLEANED")
                else:
                    final_results.append(clean_res)
            
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            if i % args.sample_interval < args.batch_size:
                print(f"\n--- [é€²æ—ãƒã‚§ãƒƒã‚¯: {i:,}è¡Œç›®] ---")
                print(f"åŸæ–‡ ({args.src_lang}): {batch_raw[0]}")
                print(f"æ´—æµ„å¾Œ: {batch_cleaned[0]}")
                print(f"ç¿»è¨³ ({args.tgt_lang}): {final_results[0]}")
                print("-" * 50)
            
            # çµæœã‚’æ›¸ãè¾¼ã¿
            for res in final_results:
                f.write(res + "\n")
            f.flush()
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            del inputs, outputs
            torch.cuda.empty_cache()
    
    print(f"\nâœ… ç¿»è¨³å®Œäº†: {args.output_file}")

if __name__ == "__main__":
    main()
