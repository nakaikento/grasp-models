#!/usr/bin/env python3
"""
MADLAD-400ã‚’ä½¿ã£ãŸTeacher Dataç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ¢ãƒ‡ãƒ«: google/madlad400-3b-mt (Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹)
"""

import argparse
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
from tqdm import tqdm
import re

def clean_translation(text: str) -> str:
    """ç¿»è¨³çµæœã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if not text or not text.strip():
        return "FAILED_TRANSLATION_CLEANED"
    
    # è¨€èªãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
    text = re.sub(r'<2[a-z]{2}>', '', text)
    text = text.strip()
    
    # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
    if not text:
        return "FAILED_TRANSLATION_CLEANED"
    
    # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆåŒã˜æ–‡å­—ãŒ10å›ä»¥ä¸Šé€£ç¶šï¼‰
    if re.search(r'(.)\1{9,}', text):
        return "FAILED_TRANSLATION_CLEANED"
    
    # åŒã˜å˜èªãŒ5å›ä»¥ä¸Šé€£ç¶š
    words = text.split()
    if len(words) > 5:
        for i in range(len(words) - 4):
            if len(set(words[i:i+5])) == 1:
                return "FAILED_TRANSLATION_CLEANED"
    
    return text

def generate_translations(
    src_file: str,
    output_file: str,
    src_lang: str = "ko",
    tgt_lang: str = "ja",
    model_name: str = "google/madlad400-3b-mt",
    batch_size: int = 16,
    num_beams: int = 4,
    max_length: int = 128,
    device: str = "cuda"
):
    """
    MADLAD-400ã§ç¿»è¨³ã‚’ç”Ÿæˆ
    
    Args:
        src_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1æ–‡ï¼‰
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
        src_lang: ã‚½ãƒ¼ã‚¹è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆko, ja, etc.ï¼‰
        tgt_lang: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã‚³ãƒ¼ãƒ‰
        model_name: MADLAD-400ãƒ¢ãƒ‡ãƒ«å
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        num_beams: Beam searchå¹…
        max_length: æœ€å¤§ç”Ÿæˆé•·
        device: ãƒ‡ãƒã‚¤ã‚¹ï¼ˆcuda/cpuï¼‰
    """
    
    print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    ).to(device)
    
    print(f"ğŸ“– å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {src_file}")
    with open(src_file, 'r', encoding='utf-8') as f:
        source_texts = [line.strip() for line in f if line.strip()]
    
    total = len(source_texts)
    print(f"ğŸ“Š ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total}")
    
    # MADLAD-400ã®è¨€èªã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: <2ko>, <2ja>, etc.
    lang_prefix = f"<2{tgt_lang}>"
    
    translations = []
    failed_count = 0
    
    with open(output_file, 'w', encoding='utf-8') as out_f:
        for i in tqdm(range(0, total, batch_size), desc="ç¿»è¨³ä¸­"):
            batch = source_texts[i:i + batch_size]
            
            # MADLAD-400ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: "<2ja> {source_text}"
            inputs = [f"{lang_prefix} {text}" for text in batch]
            
            encoded = tokenizer(
                inputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length
            ).to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **encoded,
                    max_length=max_length,
                    num_beams=num_beams,
                    early_stopping=True,
                    no_repeat_ngram_size=3,  # ç¹°ã‚Šè¿”ã—é˜²æ­¢
                    repetition_penalty=1.2   # ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
                )
            
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            
            # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            for j, translation in enumerate(decoded):
                cleaned = clean_translation(translation)
                translations.append(cleaned)
                out_f.write(cleaned + '\n')
                
                if cleaned == "FAILED_TRANSLATION_CLEANED":
                    failed_count += 1
    
    print(f"\nâœ… ç¿»è¨³å®Œäº†: {output_file}")
    print(f"ğŸ“Š çµ±è¨ˆ:")
    print(f"  - ç·æ•°: {total}")
    print(f"  - æˆåŠŸ: {total - failed_count}")
    print(f"  - å¤±æ•—: {failed_count} ({failed_count/total*100:.1f}%)")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print(f"\n--- [ã‚µãƒ³ãƒ—ãƒ«ç¿»è¨³] ---")
    for i in range(min(5, len(source_texts))):
        print(f"åŸæ–‡ ({src_lang}): {source_texts[i]}")
        print(f"ç¿»è¨³ ({tgt_lang}): {translations[i]}")
        print("-" * 50)

def main():
    parser = argparse.ArgumentParser(description="MADLAD-400ã§Teacher Dataç”Ÿæˆ")
    parser.add_argument("--src_file", required=True, help="å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--output_file", required=True, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--src_lang", default="ko", help="ã‚½ãƒ¼ã‚¹è¨€èªã‚³ãƒ¼ãƒ‰")
    parser.add_argument("--tgt_lang", default="ja", help="ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã‚³ãƒ¼ãƒ‰")
    parser.add_argument("--model_name", default="google/madlad400-3b-mt", help="ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--batch_size", type=int, default=16, help="ãƒãƒƒãƒã‚µã‚¤ã‚º")
    parser.add_argument("--num_beams", type=int, default=4, help="Beam searchå¹…")
    parser.add_argument("--max_length", type=int, default=128, help="æœ€å¤§ç”Ÿæˆé•·")
    parser.add_argument("--device", default="cuda", help="ãƒ‡ãƒã‚¤ã‚¹ï¼ˆcuda/cpuï¼‰")
    
    args = parser.parse_args()
    
    generate_translations(
        src_file=args.src_file,
        output_file=args.output_file,
        src_lang=args.src_lang,
        tgt_lang=args.tgt_lang,
        model_name=args.model_name,
        batch_size=args.batch_size,
        num_beams=args.num_beams,
        max_length=args.max_length,
        device=args.device
    )

if __name__ == "__main__":
    main()
