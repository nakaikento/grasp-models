#!/usr/bin/env python3
"""
Qwen2.5-7B + vLLM ã‚’ä½¿ç”¨ã—ãŸæ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
1. vLLMã‚µãƒ¼ãƒãƒ¼èµ·å‹•:
   python -m vllm.entrypoints.openai.api_server \
     --model Qwen/Qwen2.5-7B-Instruct --port 8000

2. æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ:
   python generate_teacher_qwen.py \
     --src_lang ko --tgt_lang ja \
     --src_file data/raw/source.ko \
     --output_file data/teacher/train.ja

â€» v2: è¡Œé †åºã‚’ä¿è¨¼ï¼ˆä¸¦åˆ—å‡¦ç†ã§ã‚‚æ­£ã—ã„é †åºã§å‡ºåŠ›ï¼‰
"""

import os
import re
import json
import time
import argparse
import requests
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# vLLM APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
VLLM_URL = "http://localhost:8000/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

# è¨€èªåãƒãƒƒãƒ”ãƒ³ã‚°
LANG_NAMES = {
    "ja": "Japanese",
    "ko": "Korean"
}

# è¨€èªæ¤œå‡ºç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
JP_PATTERN = re.compile(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ]')
KO_PATTERN = re.compile(r'[ê°€-í£]')

def contains_language(text, lang_code):
    """æŒ‡å®šã•ã‚ŒãŸè¨€èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    if lang_code == "ja":
        return bool(JP_PATTERN.search(text))
    elif lang_code == "ko":
        return bool(KO_PATTERN.search(text))
    return False

def create_prompt(src_text, src_lang, tgt_lang):
    """ç¿»è¨³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
    src_name = LANG_NAMES[src_lang]
    tgt_name = LANG_NAMES[tgt_lang]
    
    return f"""Translate the following {src_name} text to {tgt_name}. Output ONLY the translation, nothing else.

{src_name}: {src_text}

{tgt_name}:"""

def translate_single(args_tuple):
    """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ãï¼‰"""
    global_idx, src_text, src_lang, tgt_lang, timeout = args_tuple
    
    prompt = create_prompt(src_text, src_lang, tgt_lang)
    
    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 256,
        "temperature": 0.1,
    }
    
    try:
        resp = requests.post(VLLM_URL, json=payload, timeout=timeout)
        resp.raise_for_status()
        result = resp.json()["choices"][0]["message"]["content"].strip()
        
        # æ”¹è¡Œã‚’é™¤å»
        result = result.replace("\n", " ").strip()
        
        # ã‚½ãƒ¼ã‚¹è¨€èªãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯å¤±æ•—
        if contains_language(result, src_lang):
            return (global_idx, "FAILED_TRANSLATION")
        
        return (global_idx, result)
    except Exception as e:
        return (global_idx, f"ERROR: {e}")

def translate_batch_parallel_ordered(batch_with_indices, src_lang, tgt_lang, max_workers=16, timeout=30):
    """
    ä¸¦åˆ—ã§ãƒãƒƒãƒç¿»è¨³ï¼ˆé †åºä¿è¨¼ç‰ˆï¼‰
    
    Args:
        batch_with_indices: [(global_idx, text), ...] ã®ãƒªã‚¹ãƒˆ
        src_lang: ã‚½ãƒ¼ã‚¹è¨€èª
        tgt_lang: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª
        max_workers: ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
    
    Returns:
        [(global_idx, translation), ...] ã®ãƒªã‚¹ãƒˆï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
    """
    results = []
    
    # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆä½œæˆ
    tasks = [
        (global_idx, text, src_lang, tgt_lang, timeout)
        for global_idx, text in batch_with_indices
    ]
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(translate_single, task) for task in tasks]
        
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                # ã“ã®ã‚±ãƒ¼ã‚¹ã¯é€šå¸¸ç™ºç”Ÿã—ãªã„ãŒå¿µã®ãŸã‚
                pass
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x[0])
    return results

def check_vllm_server():
    """vLLMã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ç¢ºèª"""
    try:
        resp = requests.get("http://localhost:8000/health", timeout=5)
        return resp.status_code == 200
    except:
        return False

def main():
    parser = argparse.ArgumentParser(description="Qwen2.5-7B + vLLMã‚’ä½¿ç”¨ã—ãŸæ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
    
    # è¨€èªè¨­å®š
    parser.add_argument("--src_lang", type=str, required=True, 
                        choices=["ja", "ko"],
                        help="ã‚½ãƒ¼ã‚¹è¨€èª (ja: æ—¥æœ¬èª, ko: éŸ“å›½èª)")
    parser.add_argument("--tgt_lang", type=str, required=True,
                        choices=["ja", "ko"],
                        help="ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª (ja: æ—¥æœ¬èª, ko: éŸ“å›½èª)")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    parser.add_argument("--src_file", type=str, required=True,
                        help="å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--output_file", type=str, required=True,
                        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    
    # å‡¦ç†è¨­å®š
    parser.add_argument("--batch_size", type=int, default=32,
                        help="ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼‰")
    parser.add_argument("--max_workers", type=int, default=16,
                        help="ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°")
    parser.add_argument("--sample_interval", type=int, default=5000,
                        help="é€²æ—è¡¨ç¤ºã®é–“éš”")
    parser.add_argument("--limit", type=int, default=None,
                        help="å‡¦ç†è¡Œæ•°ã®åˆ¶é™ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰")
    parser.add_argument("--checkpoint_interval", type=int, default=10000,
                        help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”ï¼ˆè¡Œæ•°ï¼‰")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ Qwen2.5-7B æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (v2: é †åºä¿è¨¼)")
    print("=" * 60)
    print(f"  ã‚½ãƒ¼ã‚¹è¨€èª: {args.src_lang} ({LANG_NAMES[args.src_lang]})")
    print(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èª: {args.tgt_lang} ({LANG_NAMES[args.tgt_lang]})")
    print(f"  å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.src_file}")
    print(f"  å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.output_file}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}")
    print(f"  ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: {args.max_workers}")
    print(f"  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”: {args.checkpoint_interval}")
    print()
    
    # vLLMã‚µãƒ¼ãƒãƒ¼ç¢ºèª
    print("ğŸ” vLLMã‚µãƒ¼ãƒãƒ¼ç¢ºèªä¸­...")
    if not check_vllm_server():
        print("âŒ vLLMã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚")
        print()
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
        print("  python -m vllm.entrypoints.openai.api_server \\")
        print("    --model Qwen/Qwen2.5-7B-Instruct --port 8000 \\")
        print("    --gpu-memory-utilization 0.9")
        return 1
    print("âœ… vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šOK")
    print()
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“– å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {args.src_file}")
    if not os.path.exists(args.src_file):
        raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.src_file}")
    
    with open(args.src_file, 'r', encoding='utf-8') as f:
        src_lines = [line.strip() for line in f]
    
    total_lines = len(src_lines)
    if args.limit:
        src_lines = src_lines[:args.limit]
        total_lines = len(src_lines)
    
    print(f"âœ… {total_lines:,}è¡Œèª­ã¿è¾¼ã¿å®Œäº†")
    
    # å†é–‹å‡¦ç†: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    checkpoint_file = args.output_file + ".checkpoint"
    start_idx = 0
    all_results = []  # (idx, translation) ã®ãƒªã‚¹ãƒˆ
    
    if os.path.exists(checkpoint_file):
        print(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {checkpoint_file}")
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split('\t', 1)
                if len(parts) == 2:
                    idx, translation = int(parts[0]), parts[1]
                    all_results.append((idx, translation))
        
        if all_results:
            start_idx = max(idx for idx, _ in all_results) + 1
            print(f"âœ… {len(all_results):,}ä»¶ãƒ­ãƒ¼ãƒ‰ã€{start_idx:,}è¡Œç›®ã‹ã‚‰å†é–‹")
    else:
        os.makedirs(os.path.dirname(args.output_file) or '.', exist_ok=True)
    
    # çµ±è¨ˆ
    total_processed = len(all_results)
    total_failed = sum(1 for _, t in all_results if t.startswith("FAILED") or t.startswith("ERROR"))
    start_time = time.time()
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§é–‹ã
    checkpoint_f = open(checkpoint_file, 'a', encoding='utf-8')
    
    try:
        # ç¿»è¨³å®Ÿè¡Œ
        print(f"\nğŸ”¥ ç¿»è¨³é–‹å§‹ ({start_idx:,}è¡Œç›®ã‹ã‚‰)...")
        
        pbar = tqdm(
            range(start_idx, total_lines, args.batch_size),
            initial=start_idx // args.batch_size,
            total=(total_lines + args.batch_size - 1) // args.batch_size,
            desc="ç¿»è¨³ä¸­"
        )
        
        for batch_start in pbar:
            batch_end = min(batch_start + args.batch_size, total_lines)
            
            # (global_idx, text) ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            batch_with_indices = []
            for i in range(batch_start, batch_end):
                text = src_lines[i] if src_lines[i] else "ã€‚"
                batch_with_indices.append((i, text))
            
            # ä¸¦åˆ—ç¿»è¨³ï¼ˆé †åºä¿è¨¼ï¼‰
            results = translate_batch_parallel_ordered(
                batch_with_indices, args.src_lang, args.tgt_lang, args.max_workers
            )
            
            # çµæœã‚’ä¿å­˜
            for idx, translation in results:
                all_results.append((idx, translation))
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«å³åº§ã«æ›¸ãè¾¼ã¿
                checkpoint_f.write(f"{idx}\t{translation}\n")
                
                if translation.startswith("FAILED") or translation.startswith("ERROR"):
                    total_failed += 1
            
            checkpoint_f.flush()
            total_processed = len(all_results)
            
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            if batch_start % args.sample_interval < args.batch_size:
                elapsed = time.time() - start_time
                processed_this_run = total_processed - (start_idx if start_idx > 0 else 0)
                speed = processed_this_run / elapsed if elapsed > 0 else 0
                remaining = total_lines - total_processed
                eta = remaining / speed if speed > 0 else 0
                
                sample_idx, sample_trans = results[0] if results else (0, "N/A")
                sample_src = src_lines[sample_idx] if sample_idx < len(src_lines) else "N/A"
                
                print(f"\n--- [é€²æ—: {total_processed:,}/{total_lines:,} ({100*total_processed/total_lines:.1f}%)] ---")
                print(f"åŸæ–‡ ({args.src_lang}): {sample_src[:60]}")
                print(f"ç¿»è¨³ ({args.tgt_lang}): {sample_trans[:60]}")
                print(f"é€Ÿåº¦: {speed:.1f}è¡Œ/ç§’, å¤±æ•—: {total_failed:,}, ETA: {eta/60:.1f}åˆ†")
                print("-" * 50)
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            elapsed = time.time() - start_time
            processed_this_run = total_processed - start_idx
            speed = processed_this_run / elapsed if elapsed > 0 else 0
            pbar.set_postfix({
                "speed": f"{speed:.1f}/s",
                "failed": total_failed,
                "done": f"{total_processed:,}"
            })
    
    finally:
        checkpoint_f.close()
    
    # æœ€çµ‚å‡ºåŠ›: ã‚½ãƒ¼ãƒˆã—ã¦æ›¸ãè¾¼ã¿
    print(f"\nğŸ“ æœ€çµ‚å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­...")
    all_results.sort(key=lambda x: x[0])
    
    # æ¬ æãƒã‚§ãƒƒã‚¯
    expected_indices = set(range(total_lines))
    actual_indices = set(idx for idx, _ in all_results)
    missing = expected_indices - actual_indices
    
    if missing:
        print(f"âš ï¸ æ¬ æè¡ŒãŒ{len(missing)}ä»¶ã‚ã‚Šã¾ã™ã€‚FAILED_TRANSLATIONã§åŸ‹ã‚ã¾ã™...")
        for idx in missing:
            all_results.append((idx, "FAILED_TRANSLATION"))
        all_results.sort(key=lambda x: x[0])
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
    with open(args.output_file, 'w', encoding='utf-8') as f:
        for idx, translation in all_results:
            f.write(translation + "\n")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {checkpoint_file}")
    
    # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
    elapsed = time.time() - start_time
    print()
    print("=" * 60)
    print("âœ… ç¿»è¨³å®Œäº†")
    print("=" * 60)
    print(f"  ç·å‡¦ç†è¡Œæ•°: {len(all_results):,}")
    print(f"  å¤±æ•—æ•°: {total_failed:,} ({100*total_failed/len(all_results):.1f}%)")
    print(f"  æ‰€è¦æ™‚é–“: {elapsed/60:.1f}åˆ†")
    print(f"  å¹³å‡é€Ÿåº¦: {(len(all_results) - start_idx)/elapsed:.1f}è¡Œ/ç§’")
    print(f"  å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.output_file}")
    print()
    
    # æ¤œè¨¼
    print("ğŸ” ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆæ¤œè¨¼...")
    with open(args.output_file, 'r', encoding='utf-8') as f:
        output_lines = f.readlines()
    
    if len(output_lines) == total_lines:
        print(f"âœ… è¡Œæ•°ä¸€è‡´: {len(output_lines):,} = {total_lines:,}")
    else:
        print(f"âŒ è¡Œæ•°ä¸ä¸€è‡´: å‡ºåŠ›{len(output_lines):,} != å…¥åŠ›{total_lines:,}")
    
    return 0

if __name__ == "__main__":
    exit(main())
