#!/usr/bin/env python3
"""
Qwen2.5-72B-AWQ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

vLLM + AWQé‡å­åŒ–ã§é«˜å“è³ªãªéŸ“å›½èªâ†’æ—¥æœ¬èªç¿»è¨³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚
ç´„100ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã®å¤§è¦æ¨¡ç”Ÿæˆã‚’æƒ³å®šã€‚

ä½¿ã„æ–¹:
    # åŸºæœ¬å®Ÿè¡Œï¼ˆAI Hubå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    python generate_qwen72b_awq.py \
        --input /path/to/aihub \
        --output /path/to/output/teacher_data.jsonl

    # ã‚µãƒ³ãƒ—ãƒ«æ•°æŒ‡å®š
    python generate_qwen72b_awq.py \
        --input /path/to/aihub \
        --output output.jsonl \
        --limit 100000

    # å†é–‹ï¼ˆå‰å›ã®ç¶šãã‹ã‚‰ï¼‰
    python generate_qwen72b_awq.py \
        --input /path/to/aihub \
        --output output.jsonl \
        --resume

å¿…è¦ç’°å¢ƒ:
    - NVIDIA GPU (RTX A6000 48GB+ æ¨å¥¨)
    - vLLM 0.15.0+
    - ç’°å¢ƒå¤‰æ•°: HF_HOME, XDG_CACHE_HOME ã‚’ /workspace ç­‰ã«è¨­å®š

æ¨å®šå‡¦ç†æ™‚é–“:
    - RTX A6000 (batch=128): ~12-14 samples/s â†’ 100ä¸‡ä»¶ â‰ˆ 20æ™‚é–“
"""

import argparse
import json
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Generator

try:
    from vllm import LLM, SamplingParams
except ImportError:
    print("âŒ vLLMãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("   pip install vllm")
    exit(1)

from tqdm import tqdm


# === è¨­å®š ===
MODEL_ID = "Qwen/Qwen2.5-72B-Instruct-AWQ"

# Few-shotä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè©•ä¾¡ã§æœ€ã‚‚åŠ¹æœçš„ã ã£ãŸã‚‚ã®ï¼‰
SYSTEM_PROMPT = """ã‚ãªãŸã¯éŸ“å›½ãƒ‰ãƒ©ãƒãƒ»æ˜ ç”»ãƒ»ã‚¢ãƒ‹ãƒ¡ã®å­—å¹•ç¿»è¨³ã‚’å°‚é–€ã¨ã™ã‚‹ç¿»è¨³è€…ã§ã™ã€‚
è¦–è´è€…ãŒç”»é¢ã‚’è¦‹ãªãŒã‚‰è‡ªç„¶ã«ç†è§£ã§ãã‚‹å­—å¹•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ç¿»è¨³æ–¹é‡ã€‘
- éŸ“å›½èªã®æ„å‘³ã¨ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’æ­£ç¢ºã«ä¼ãˆã‚‹è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³
- æ–‡åŒ–çš„ãªèƒŒæ™¯ã‚’è€ƒæ…®ã—ã€æ—¥æœ¬äººè¦–è´è€…ã«é•å’Œæ„Ÿãªãä¼ã‚ã‚‹è¡¨ç¾ã‚’ä½¿ç”¨
- æ•¬èªãƒ»ã‚¿ãƒ¡å£ã®ãƒ¬ãƒ™ãƒ«ã¯åŸæ–‡ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ã«åˆã‚ã›ã‚‹
- å­—å¹•ã¨ã—ã¦èª­ã¿ã‚„ã™ã„ç°¡æ½”ãªè¡¨ç¾ã‚’å¿ƒãŒã‘ã‚‹

ã€å³å®ˆäº‹é …ã€‘
- ç¿»è¨³æ–‡ã®ã¿ã‚’å‡ºåŠ›ï¼ˆèª¬æ˜ãƒ»è£œè¶³ã¯ä¸è¦ï¼‰
- é€šè²¨ãƒ»å˜ä½ã¯ãã®ã¾ã¾ç¶­æŒï¼ˆã‚¦ã‚©ãƒ³â†’å††ã¸ã®å¤‰æ›ç¦æ­¢ï¼‰
- å›ºæœ‰åè©ã¯åŸéŸ³ã«è¿‘ã„ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜
- æ—¥æœ¬èªã®ã¿ã§å‡ºåŠ›ï¼ˆä¸­å›½èªæ··å…¥ç¦æ­¢ï¼‰

ã€ç¿»è¨³ä¾‹ã€‘
éŸ“: ê²½ê¸° ë‹¹ì¼ì—ëŠ” ë‚ ì”¨ê°€ ì¢‹ë„¤.
æ—¥: ç«¶æŠ€å½“æ—¥ã«ã¯å¤©æ°—ãŒã„ã„ã­ã€‚

éŸ“: ë‹¹ì‹ ì€ ë¬´ìŠ¨ ë§› ì•„ì´ìŠ¤í¬ë¦¼ì„ ì›í•˜ë‚˜ìš”?
æ—¥: ã‚ãªãŸã¯ä½•å‘³ã®ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ãŒã»ã—ã„ã§ã™ã‹ï¼Ÿ

éŸ“: ê³ ê°ë‹˜ ì±…ì œëª©ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ë°”ë¡œ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
æ—¥: ãŠå®¢æ§˜ã€æœ¬ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãŠã£ã—ã‚ƒã£ã¦ã„ãŸã ã‘ã‚Œã°ã€ã™ãã«ã”æ¡ˆå†…ã—ã¾ã™ã€‚"""


def load_source_data(input_path: Path, limit: int = None) -> Generator[dict, None, None]:
    """
    ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
    
    å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
    - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ko_reference.txt + ja_source.txt): AI Hubå½¢å¼
    - .jsonl ãƒ•ã‚¡ã‚¤ãƒ«: {"ko": "...", "ja": "..."} å½¢å¼
    - .txt ãƒ•ã‚¡ã‚¤ãƒ«: 1è¡Œ1æ–‡ï¼ˆéŸ“å›½èªã®ã¿ï¼‰
    """
    input_path = Path(input_path)
    count = 0
    
    if input_path.is_dir():
        # AI Hubå½¢å¼ï¼ˆtxtä¸¦åˆ—ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        ko_file = input_path / "ko_reference.txt"
        ja_file = input_path / "ja_source.txt"
        
        if ko_file.exists():
            with open(ko_file, "r", encoding="utf-8") as f_ko:
                ja_lines = None
                if ja_file.exists():
                    with open(ja_file, "r", encoding="utf-8") as f_ja:
                        ja_lines = f_ja.readlines()
                
                for i, ko_line in enumerate(f_ko):
                    if limit and count >= limit:
                        return
                    
                    ko_text = ko_line.strip()
                    if not ko_text:
                        continue
                    
                    item = {"ko": ko_text, "idx": count}
                    if ja_lines and i < len(ja_lines):
                        item["ja_ref"] = ja_lines[i].strip()
                    
                    yield item
                    count += 1
    
    elif input_path.suffix == ".jsonl":
        with open(input_path, "r", encoding="utf-8") as f:
            for line in f:
                if limit and count >= limit:
                    return
                
                item = json.loads(line.strip())
                item["idx"] = count
                yield item
                count += 1
    
    elif input_path.suffix == ".txt":
        with open(input_path, "r", encoding="utf-8") as f:
            for line in f:
                if limit and count >= limit:
                    return
                
                ko_text = line.strip()
                if not ko_text:
                    continue
                
                yield {"ko": ko_text, "idx": count}
                count += 1
    
    else:
        raise ValueError(f"Unsupported input format: {input_path}")


def count_source_lines(input_path: Path) -> int:
    """ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    input_path = Path(input_path)
    
    if input_path.is_dir():
        ko_file = input_path / "ko_reference.txt"
        if ko_file.exists():
            with open(ko_file, "r", encoding="utf-8") as f:
                return sum(1 for line in f if line.strip())
    elif input_path.exists():
        with open(input_path, "r", encoding="utf-8") as f:
            return sum(1 for line in f if line.strip())
    
    return 0


def get_processed_count(output_path: Path) -> int:
    """æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å–å¾—"""
    if not output_path.exists():
        return 0
    
    with open(output_path, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)


def build_prompt(ko_text: str) -> str:
    """ç¿»è¨³ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆQwen chat formatï¼‰"""
    return f"<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n{ko_text}<|im_end|>\n<|im_start|>assistant\n"


def main():
    parser = argparse.ArgumentParser(
        description="Qwen2.5-72B-AWQ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
    # åŸºæœ¬å®Ÿè¡Œ
    python generate_qwen72b_awq.py -i data/aihub -o output/teacher.jsonl
    
    # 10ä¸‡ä»¶ã®ã¿ç”Ÿæˆ
    python generate_qwen72b_awq.py -i data/aihub -o output/teacher.jsonl -n 100000
    
    # ä¸­æ–­å¾Œã®å†é–‹
    python generate_qwen72b_awq.py -i data/aihub -o output/teacher.jsonl --resume
"""
    )
    parser.add_argument("--input", "-i", type=str, required=True,
                        help="å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª or ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
    parser.add_argument("--output", "-o", type=str, required=True,
                        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.jsonl)")
    parser.add_argument("--limit", "-n", type=int, default=None,
                        help="æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°")
    parser.add_argument("--batch-size", "-b", type=int, default=128,
                        help="ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰")
    parser.add_argument("--resume", "-r", action="store_true",
                        help="å‰å›ã®ç¶šãã‹ã‚‰å†é–‹")
    parser.add_argument("--model", "-m", type=str, default=MODEL_ID,
                        help=f"ãƒ¢ãƒ‡ãƒ«IDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {MODEL_ID}ï¼‰")
    parser.add_argument("--gpu-memory", type=float, default=0.9,
                        help="GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.9ï¼‰")
    parser.add_argument("--max-tokens", type=int, default=256,
                        help="æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰")
    args = parser.parse_args()

    input_path = Path(args.input)
    output_path = Path(args.output)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_path.parent.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("ğŸ“š Qwen2.5-72B-AWQ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«:       {args.model}")
    print(f"å…¥åŠ›:         {input_path}")
    print(f"å‡ºåŠ›:         {output_path}")
    print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}")
    print(f"GPUãƒ¡ãƒ¢ãƒª:    {args.gpu_memory * 100:.0f}%")
    print()

    # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    total_lines = count_source_lines(input_path)
    if args.limit:
        total_lines = min(total_lines, args.limit)
    print(f"ğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: {total_lines:,} ä»¶")

    # å†é–‹ãƒ¢ãƒ¼ãƒ‰
    skip_count = 0
    if args.resume:
        skip_count = get_processed_count(output_path)
        if skip_count > 0:
            print(f"â© å†é–‹ãƒ¢ãƒ¼ãƒ‰: {skip_count:,} ä»¶ã‚¹ã‚­ãƒƒãƒ—")
    
    remaining = total_lines - skip_count
    if remaining <= 0:
        print("âœ… æ—¢ã«å®Œäº†ã—ã¦ã„ã¾ã™")
        return
    
    print(f"ğŸ“ å‡¦ç†å¯¾è±¡: {remaining:,} ä»¶")
    print()

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    start_load = time.time()
    
    llm = LLM(
        model=args.model,
        quantization="awq",
        dtype="float16",
        gpu_memory_utilization=args.gpu_memory,
        max_model_len=2048,
        enforce_eager=True,  # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡å•é¡Œå›é¿ï¼ˆtorch.compileç„¡åŠ¹åŒ–ï¼‰
    )
    
    load_time = time.time() - start_load
    print(f"   å®Œäº† ({load_time:.1f}ç§’)")
    print()

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    sampling_params = SamplingParams(
        temperature=0.0,  # Greedy decodingï¼ˆä¸€è²«æ€§ã®ãŸã‚ï¼‰
        max_tokens=args.max_tokens,
        stop=["<|im_end|>", "\n\n"],
    )

    # å‡¦ç†é–‹å§‹
    print("ğŸ”„ ç¿»è¨³ç”Ÿæˆä¸­...")
    start_time = time.time()
    processed = 0
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§é–‹ã
    mode = "a" if args.resume and skip_count > 0 else "w"
    
    with open(output_path, mode, encoding="utf-8") as f_out:
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒã§å‡¦ç†
        batch = []
        batch_items = []
        
        data_iter = load_source_data(input_path, args.limit)
        
        # å†é–‹æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
        for _ in range(skip_count):
            next(data_iter, None)
        
        pbar = tqdm(total=remaining, desc="ç”Ÿæˆä¸­", unit="samples")
        
        for item in data_iter:
            batch.append(build_prompt(item["ko"]))
            batch_items.append(item)
            
            if len(batch) >= args.batch_size:
                # ãƒãƒƒãƒå‡¦ç†
                outputs = llm.generate(batch, sampling_params)
                
                for item, output in zip(batch_items, outputs):
                    ja_text = output.outputs[0].text.strip()
                    
                    result = {
                        "ko": item["ko"],
                        "ja": ja_text,
                    }
                    if "ja_ref" in item:
                        result["ja_ref"] = item["ja_ref"]
                    
                    f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
                    processed += 1
                
                pbar.update(len(batch))
                f_out.flush()  # å®šæœŸçš„ã«ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                
                # é€Ÿåº¦è¡¨ç¤ºæ›´æ–°
                elapsed = time.time() - start_time
                speed = processed / elapsed if elapsed > 0 else 0
                eta_seconds = (remaining - processed) / speed if speed > 0 else 0
                pbar.set_postfix({
                    "speed": f"{speed:.1f}/s",
                    "ETA": str(timedelta(seconds=int(eta_seconds)))
                })
                
                batch = []
                batch_items = []
        
        # æ®‹ã‚Šã‚’å‡¦ç†
        if batch:
            outputs = llm.generate(batch, sampling_params)
            
            for item, output in zip(batch_items, outputs):
                ja_text = output.outputs[0].text.strip()
                
                result = {
                    "ko": item["ko"],
                    "ja": ja_text,
                }
                if "ja_ref" in item:
                    result["ja_ref"] = item["ja_ref"]
                
                f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
                processed += 1
            
            pbar.update(len(batch))
        
        pbar.close()

    # çµæœã‚µãƒãƒªãƒ¼
    elapsed = time.time() - start_time
    speed = processed / elapsed if elapsed > 0 else 0
    
    print()
    print("=" * 60)
    print("ğŸ“Š å®Œäº†ã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"å‡¦ç†ä»¶æ•°:     {processed:,} ä»¶")
    print(f"å‡¦ç†æ™‚é–“:     {timedelta(seconds=int(elapsed))}")
    print(f"é€Ÿåº¦:         {speed:.2f} samples/s")
    print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {output_path.stat().st_size / 1024 / 1024:.1f} MB")
    print("=" * 60)

    # æ¨å®šæ™‚é–“ï¼ˆ100ä¸‡ä»¶ã®å ´åˆï¼‰
    if processed > 0:
        est_1m = 1_000_000 / speed / 3600
        print(f"\nğŸ’¡ 100ä¸‡ä»¶ã®æ¨å®šå‡¦ç†æ™‚é–“: {est_1m:.1f} æ™‚é–“")


if __name__ == "__main__":
    main()
