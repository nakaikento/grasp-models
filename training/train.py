import os
import torch
import argparse
from pathlib import Path
from datasets import Dataset, DatasetDict
from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    EarlyStoppingCallback
)
import evaluate
import numpy as np

# --- è¨­å®šé …ç›® ---
MODEL_NAME = "Helsinki-NLP/opus-ja-ko"  # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
DATA_JA = "data/clean/train.ja"
DATA_KO = "data/clean/train.ko"
OUTPUT_DIR = "models/marian_ja_ko_v1"

def load_and_split_data():
    """ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦å­¦ç¿’ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²"""
    print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {DATA_JA} / {DATA_KO}")
    with open(DATA_JA, 'r', encoding='utf-8') as f:
        ja_lines = [line.strip() for line in f]
    with open(DATA_KO, 'r', encoding='utf-8') as f:
        ko_lines = [line.strip() for line in f]
    
    # Datasetä½œæˆ
    full_dataset = Dataset.from_dict({
        "ja": ja_lines,
        "ko": ko_lines
    })
    
    # 5%ã‚’æ¤œè¨¼ç”¨ã«åˆ†å‰² (ç´„3ä¸‡è¡Œ)
    return full_dataset.train_test_split(test_size=0.05, seed=42)

def compute_metrics(eval_preds, tokenizer, metric):
    """BLEUã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    
    # -100 (ãƒ©ãƒ™ãƒ«ç„¡è¦–ç”¨) ã‚’ pad ã«æˆ»ã™
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # SacreBLEUç”¨ã«æ•´å½¢
    decoded_labels = [[line] for line in decoded_labels]
    
    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch_size", type=int, default=128) # 4090ãªã‚‰128ã§å®‰å®š
    parser.add_argument("--epochs", type=int, default=5)       # 60ä¸‡è¡Œãªã‚‰3~5ã‚¨ãƒãƒƒã‚¯
    parser.add_argument("--lr", type=float, default=3e-5)
    args = parser.parse_args()

    # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ & ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    print(f"ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    metric = evaluate.load("sacrebleu")

    # 2. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    dataset = load_and_split_data()
    
    def preprocess_function(examples):
        inputs = examples["ja"]
        targets = examples["ko"]
        model_inputs = tokenizer(inputs, max_length=128, truncation=True)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        with tokenizer.as_target_tokenizer():
            labels = tokenizer(targets, max_length=128, truncation=True)
        
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    print("âš¡ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Ÿè¡Œä¸­ (ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹)...")
    tokenized_datasets = dataset.map(
        preprocess_function, 
        batched=True, 
        num_proc=8, # CPUã‚³ã‚¢æ•°ã«åˆã‚ã›ã¦èª¿æ•´
        remove_columns=dataset["train"].column_names
    )

    # 3. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­: {MODEL_NAME}")
    config = AutoConfig.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, config=config)

    # 4. å­¦ç¿’å¼•æ•°ã®è¨­å®š
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="steps",
        eval_steps=2000,               # 2000ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è©•ä¾¡
        save_strategy="steps",
        save_steps=2000,
        logging_steps=500,
        learning_rate=args.lr,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=args.epochs,
        predict_with_generate=True,
        # 4090 æœ€é©åŒ–è¨­å®š
        bf16=True,                     # Ampere/Ada GPUãªã‚‰å¿…é ˆ
        fp16=False,
        gradient_checkpointing=False,  # Marianã¯è»½ã„ã®ã§Falseã§OK
        dataloader_num_workers=4,
        load_best_model_at_end=True,
        metric_for_best_model="bleu",
        greater_is_better=True,
        warmup_steps=1000,
    )

    # 5. ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®æ§‹ç¯‰
    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
    
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda x: compute_metrics(x, tokenizer, metric),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # BLEUãŒæ”¹å–„ã—ãªããªã£ãŸã‚‰åœæ­¢
    )

    # 6. å­¦ç¿’å®Ÿè¡Œ
    print("ğŸš€ å­¦ç¿’é–‹å§‹ï¼")
    trainer.train()

    # 7. ä¿å­˜
    print(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­: {OUTPUT_DIR}/final")
    trainer.save_model(os.path.join(OUTPUT_DIR, "final"))
    print("âœ… å…¨ã¦ã®å·¥ç¨‹ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()