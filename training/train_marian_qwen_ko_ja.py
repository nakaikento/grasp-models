#!/usr/bin/env python3
"""
MarianMT KOâ†’JA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Qwenæ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦MarianMTã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- è‹±èªæ··å…¥ãƒ»ã‚¢ãƒ©ãƒ“ã‚¢æ–‡å­—ãªã©ã®ä½å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- Knowledge Distillationæ–¹å¼

ä½¿ç”¨æ–¹æ³•:
    python train_marian_ko_ja.py \
        --src_file ../data/raw/OpenSubtitles.ja-ko.ko \
        --tgt_file ../data/teacher/qwen_train.ja \
        --output_dir ../models/marian-ko-ja-finetuned \
        --epochs 3
"""

import argparse
import logging
import os
import re
import sys
from dataclasses import dataclass
from typing import List, Tuple, Optional

import torch
from datasets import Dataset
from transformers import (
    MarianMTModel,
    MarianTokenizer,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback,
)
import evaluate

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('training.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class FilterStats:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆ"""
    total: int = 0
    pure_english: int = 0
    english_mixed: int = 0
    arabic_chars: int = 0
    empty_lines: int = 0
    too_long: int = 0
    passed: int = 0


class DataFilter:
    """ä½å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    
    # ç´”è‹±èªè¡Œã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    PURE_ENGLISH_PATTERN = re.compile(r'^[A-Za-z0-9\s.,!?\-\'"()]+$')
    
    # 5æ–‡å­—ä»¥ä¸Šã®é€£ç¶šè‹±èªï¼ˆå›ºæœ‰åè©ä»¥å¤–ï¼‰
    LONG_ENGLISH_PATTERN = re.compile(r'[A-Za-z]{5,}')
    
    # ã‚¢ãƒ©ãƒ“ã‚¢æ–‡å­—
    ARABIC_PATTERN = re.compile(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FFØŸ]')
    
    # è¨±å¯ã™ã‚‹è‹±èªï¼ˆå›ºæœ‰åè©ã€ä¸€èˆ¬çš„ãªç•¥èªï¼‰
    ALLOWED_ENGLISH = {
        'OK', 'TV', 'CD', 'DVD', 'PC', 'FBI', 'CIA', 'DNA', 'GPS', 'VIP',
        'iPhone', 'iPad', 'Google', 'Facebook', 'Twitter', 'YouTube',
        'Mr', 'Mrs', 'Dr', 'Jr', 'Sr', 'vs', 'etc', 'No', 'OK',
        'LOVE', 'HAPPY', 'NEW', 'GOOD', 'BAD', 'THE', 'AND', 'FOR',
    }
    
    def __init__(self, max_length: int = 256):
        self.max_length = max_length
        self.stats = FilterStats()
    
    def is_valid(self, src: str, tgt: str) -> bool:
        """ãƒšã‚¢ãŒæœ‰åŠ¹ã‹ã©ã†ã‹åˆ¤å®š"""
        self.stats.total += 1
        
        # ç©ºè¡Œãƒã‚§ãƒƒã‚¯
        if not src.strip() or not tgt.strip():
            self.stats.empty_lines += 1
            return False
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ—¥æœ¬èªï¼‰ã®ãƒã‚§ãƒƒã‚¯
        tgt = tgt.strip()
        
        # ç´”è‹±èªè¡Œ
        if self.PURE_ENGLISH_PATTERN.match(tgt):
            self.stats.pure_english += 1
            return False
        
        # ã‚¢ãƒ©ãƒ“ã‚¢æ–‡å­—
        if self.ARABIC_PATTERN.search(tgt):
            self.stats.arabic_chars += 1
            return False
        
        # é•·ã„è‹±èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ï¼ˆè¨±å¯ãƒªã‚¹ãƒˆä»¥å¤–ï¼‰
        english_matches = self.LONG_ENGLISH_PATTERN.findall(tgt)
        if english_matches:
            # è¨±å¯ãƒªã‚¹ãƒˆã«ãªã„è‹±èªãŒã‚ã‚Œã°ãƒ•ã‚£ãƒ«ã‚¿
            non_allowed = [m for m in english_matches if m.upper() not in self.ALLOWED_ENGLISH]
            if non_allowed:
                self.stats.english_mixed += 1
                return False
        
        # é•·ã™ãã‚‹æ–‡
        if len(src) > self.max_length or len(tgt) > self.max_length:
            self.stats.too_long += 1
            return False
        
        self.stats.passed += 1
        return True
    
    def report(self) -> str:
        """çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ"""
        s = self.stats
        pass_rate = (s.passed / s.total * 100) if s.total > 0 else 0
        return f"""
========================================
ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
========================================
ç·è¡Œæ•°:         {s.total:,}
----------------------------------------
ç´”è‹±èªè¡Œ:       {s.pure_english:,} ({s.pure_english/s.total*100:.2f}%)
è‹±èªæ··å…¥:       {s.english_mixed:,} ({s.english_mixed/s.total*100:.2f}%)
ã‚¢ãƒ©ãƒ“ã‚¢æ–‡å­—:   {s.arabic_chars:,} ({s.arabic_chars/s.total*100:.2f}%)
ç©ºè¡Œ:           {s.empty_lines:,} ({s.empty_lines/s.total*100:.2f}%)
é•·ã™ãã‚‹æ–‡:     {s.too_long:,} ({s.too_long/s.total*100:.2f}%)
----------------------------------------
âœ… é€šé:        {s.passed:,} ({pass_rate:.2f}%)
========================================
"""


def load_and_filter_data(
    src_file: str,
    tgt_file: str,
    max_length: int = 256,
    val_ratio: float = 0.01,
) -> Tuple[Dataset, Dataset, FilterStats]:
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦åˆ†å‰²"""
    
    logger.info(f"ğŸ“‚ ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {src_file}")
    logger.info(f"ğŸ“‚ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {tgt_file}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(src_file, 'r', encoding='utf-8') as f:
        src_lines = f.readlines()
    with open(tgt_file, 'r', encoding='utf-8') as f:
        tgt_lines = f.readlines()
    
    assert len(src_lines) == len(tgt_lines), \
        f"è¡Œæ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“: src={len(src_lines)}, tgt={len(tgt_lines)}"
    
    logger.info(f"ğŸ“Š ç·è¡Œæ•°: {len(src_lines):,}")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    data_filter = DataFilter(max_length=max_length)
    filtered_pairs = []
    
    for src, tgt in zip(src_lines, tgt_lines):
        src = src.strip()
        tgt = tgt.strip()
        if data_filter.is_valid(src, tgt):
            filtered_pairs.append({'source': src, 'target': tgt})
    
    logger.info(data_filter.report())
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦åˆ†å‰²
    import random
    random.seed(42)
    random.shuffle(filtered_pairs)
    
    val_size = int(len(filtered_pairs) * val_ratio)
    train_data = filtered_pairs[val_size:]
    val_data = filtered_pairs[:val_size]
    
    logger.info(f"ğŸ“š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_data):,}")
    logger.info(f"ğŸ“š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_data):,}")
    
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data)
    
    return train_dataset, val_dataset, data_filter.stats


def preprocess_function(examples, tokenizer, max_length=128):
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‡¦ç†"""
    inputs = examples['source']
    targets = examples['target']
    
    model_inputs = tokenizer(
        inputs,
        max_length=max_length,
        truncation=True,
        padding='max_length',
    )
    
    labels = tokenizer(
        text_target=targets,
        max_length=max_length,
        truncation=True,
        padding='max_length',
    )
    
    model_inputs['labels'] = labels['input_ids']
    return model_inputs


def compute_metrics(eval_preds, tokenizer, metric_bleu, metric_chrf):
    """è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
    preds, labels = eval_preds
    
    # -100ã‚’pad_token_idã«ç½®æ›
    labels = [[l if l != -100 else tokenizer.pad_token_id for l in label] for label in labels]
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # BLEU
    bleu_result = metric_bleu.compute(
        predictions=decoded_preds,
        references=[[l] for l in decoded_labels]
    )
    
    # chrF++
    chrf_result = metric_chrf.compute(
        predictions=decoded_preds,
        references=[[l] for l in decoded_labels],
        word_order=2  # chrF++
    )
    
    return {
        'bleu': bleu_result['bleu'] * 100,
        'chrf': chrf_result['score'],
    }


def main():
    parser = argparse.ArgumentParser(description='MarianMT KOâ†’JA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°')
    parser.add_argument('--src_file', type=str, required=True,
                        help='ã‚½ãƒ¼ã‚¹ï¼ˆéŸ“å›½èªï¼‰ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--tgt_file', type=str, required=True,
                        help='ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ—¥æœ¬èªï¼‰ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output_dir', type=str, default='../models/marian-ko-ja-finetuned',
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--base_model', type=str, default='Helsinki-NLP/opus-mt-ko-ja',
                        help='ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«')
    parser.add_argument('--epochs', type=int, default=3,
                        help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--learning_rate', type=float, default=5e-5,
                        help='å­¦ç¿’ç‡')
    parser.add_argument('--max_length', type=int, default=128,
                        help='æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·')
    parser.add_argument('--warmup_ratio', type=float, default=0.1,
                        help='ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¯”ç‡')
    parser.add_argument('--val_ratio', type=float, default=0.01,
                        help='æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ¯”ç‡')
    parser.add_argument('--fp16', action='store_true',
                        help='æ··åˆç²¾åº¦å­¦ç¿’')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,
                        help='å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—')
    
    args = parser.parse_args()
    
    # GPUç¢ºèª
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸ Device: {device}")
    if torch.cuda.is_available():
        logger.info(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ–¥ï¸ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    train_dataset, val_dataset, filter_stats = load_and_filter_data(
        args.src_file,
        args.tgt_file,
        max_length=args.max_length * 4,  # æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã§ã–ã£ãã‚Š
        val_ratio=args.val_ratio,
    )
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    logger.info(f"ğŸ“¦ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {args.base_model}")
    tokenizer = MarianTokenizer.from_pretrained(args.base_model)
    model = MarianMTModel.from_pretrained(args.base_model)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    logger.info("ğŸ”„ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºä¸­...")
    train_dataset = train_dataset.map(
        lambda x: preprocess_function(x, tokenizer, args.max_length),
        batched=True,
        remove_columns=['source', 'target'],
        desc="Tokenizing train",
    )
    val_dataset = val_dataset.map(
        lambda x: preprocess_function(x, tokenizer, args.max_length),
        batched=True,
        remove_columns=['source', 'target'],
        desc="Tokenizing val",
    )
    
    # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    metric_bleu = evaluate.load('bleu')
    metric_chrf = evaluate.load('chrf')
    
    # Data Collator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
    )
    
    # å­¦ç¿’è¨­å®š
    training_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size * 2,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_ratio=args.warmup_ratio,
        weight_decay=0.01,
        fp16=args.fp16 and torch.cuda.is_available(),
        logging_steps=100,
        eval_strategy='steps',
        eval_steps=1000,
        save_strategy='steps',
        save_steps=1000,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model='chrf',
        greater_is_better=True,
        predict_with_generate=True,
        generation_max_length=args.max_length,
        report_to='none',  # wandbç„¡åŠ¹
        dataloader_num_workers=4,
    )
    
    # Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda x: compute_metrics(x, tokenizer, metric_bleu, metric_chrf),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
    )
    
    # å­¦ç¿’é–‹å§‹
    logger.info("ğŸš€ å­¦ç¿’é–‹å§‹!")
    logger.info(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {args.epochs}")
    logger.info(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}")
    logger.info(f"   å­¦ç¿’ç‡: {args.learning_rate}")
    logger.info(f"   ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(train_dataset) // args.batch_size * args.epochs:,}")
    
    trainer.train()
    
    # ä¿å­˜
    logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {args.output_dir}")
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    
    # æœ€çµ‚è©•ä¾¡
    logger.info("ğŸ“Š æœ€çµ‚è©•ä¾¡...")
    eval_results = trainer.evaluate()
    logger.info(f"   BLEU: {eval_results.get('eval_bleu', 0):.2f}")
    logger.info(f"   chrF++: {eval_results.get('eval_chrf', 0):.2f}")
    
    logger.info("âœ… å­¦ç¿’å®Œäº†!")


if __name__ == '__main__':
    main()
